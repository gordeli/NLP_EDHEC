{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gordeli/NLP_EDHEC/blob/main/colab/01_Text_Processing_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "APfI_c8B40Vn"
   },
   "source": [
    "#Natural Language Processing @ EDHEC, 2022\n",
    "\n",
    "# Part 1: Text Processing Basics\n",
    "\n",
    "[-> Data Collection](https://colab.research.google.com/github/gordeli/NLP_EDHEC/blob/main/colab/02_Data_Collection.ipynb)\n",
    "\n",
    "Dates: January 31 - February 11, 2022\n",
    "\n",
    "Facilitator: [Ivan Gordeliy](https://www.linkedin.com/in/gordeli/)\n",
    "\n",
    "(To edit this notebook: File -> Open in Playground Mode)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "cdTajgZhkGWX"
   },
   "outputs": [],
   "source": [
    "## Initial Setup\n",
    "\n",
    "- **Run \"Setup\" below first.**\n",
    "\n",
    "    - This will load libraries and download some resources that we'll use throughout the tutorial.\n",
    "\n",
    "    - You will see a message reading \"Done with setup!\" when this process completes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GKVEnPi34qj4"
   },
   "outputs": [],
   "source": [
    "#@title Setup (click the \"run\" button to the left) {display-mode: \"form\"}\n",
    "\n",
    "## Setup ##\n",
    "\n",
    "# imports\n",
    "\n",
    "# built-in Python libraries\n",
    "# -------------------------\n",
    "\n",
    "# counting and data management\n",
    "import collections\n",
    "# operating system utils\n",
    "import os\n",
    "# regular expressions\n",
    "import re\n",
    "# additional string functions\n",
    "import string\n",
    "# system utilities\n",
    "import sys\n",
    "# request() will be used to load web content\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "# 3rd party libraries\n",
    "# -------------------\n",
    "\n",
    "# Natural Language Toolkit (https://www.nltk.org/)\n",
    "import nltk\n",
    "\n",
    "# download punctuation related NLTK functions\n",
    "# (needed for sent_tokenize())\n",
    "nltk.download('punkt')\n",
    "# download NLKT part-of-speech tagger\n",
    "# (needed for pos_tag())\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# download wordnet\n",
    "# (needed for lemmatization)\n",
    "nltk.download('wordnet')\n",
    "# download stopword lists\n",
    "# (needed for stopword removal)\n",
    "nltk.download('stopwords')\n",
    "# dictionary of English words\n",
    "nltk.download('words')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# numpy: matrix library for Python\n",
    "import numpy as np\n",
    "\n",
    "# scipy: scientific operations\n",
    "# works with numpy objects\n",
    "import scipy\n",
    "\n",
    "# matplotlib (and pyplot) for visualizations\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn for basic machine learning operations\n",
    "import sklearn\n",
    "import sklearn.manifold\n",
    "import sklearn.cluster\n",
    "\n",
    "# worldcloud tool\n",
    "!pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# for checking object memory usage\n",
    "!pip install pympler\n",
    "from pympler import asizeof\n",
    "\n",
    "!pip install spacy\n",
    "import spacy\n",
    "\n",
    "# Downloading data\n",
    "# ----------------\n",
    "if not os.path.exists(\"aclImdb\"):\n",
    "    !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    !tar -xzf aclImdb_v1.tar.gz\n",
    "\n",
    "print()\n",
    "print(\"Done with setup!\")\n",
    "print(\"If you'd like, you can click the (X) button to the left to clear this output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4sK9riH96uta"
   },
   "source": [
    "---\n",
    "## Basic Text Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UepTkw-gjzAn"
   },
   "source": [
    "\n",
    "### Built-in Python functions\n",
    "\n",
    "- Basic Python fuctions provide a good starting place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YHQ2OyBNtkf7"
   },
   "source": [
    "- First, we should try to split a sentence into individual words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MWkjtSPv8H7s"
   },
   "outputs": [],
   "source": [
    "text = \"EDHEC Business School (French: Ecole des Hautes Etudes Commerciales du Nord) is a French business school. As a Grande Ã©cole in France, it specializes in business and management studies. It has five campuses: Lille, Nice, Paris, London, and Singapore,[4] and offers undergraduate (BBA), graduate (MSc and MiM), executive education (Global MBA, EMBA) PhD in Finance, and a variety of open and customized programmes.[5] It has 8,000 students enrolled in traditional graduate and undergraduate programmes, 150 partner universities and a network of more than 40,000 alumni in over 125 countries.\"\n",
    "\n",
    "# We can split on all whitespace with split()\n",
    "words = text.split()\n",
    "print(\"WORDS:\",words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1cdH4AJKBaYx"
   },
   "source": [
    "- It is fairly straightforward to do things like remove punctuation, lowercase, or access individual letters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E4IKHf5j_nX7"
   },
   "outputs": [],
   "source": [
    "# for the first 10 words\n",
    "for word in words [:10]:\n",
    "    \n",
    "    # print the string \"word:\", the word itself, \n",
    "    # and end with a veritcal bar character instead of a newline\n",
    "    print(\"word:\", word, end=' | ')\n",
    "    \n",
    "    # strip removes characters at the beginning and end of a string\n",
    "    # string.punctuation contains: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "    print(\"no punctuation:\", word.strip(string.punctuation), end=' | ')\n",
    "    \n",
    "    # lower() and upper() change case\n",
    "    print(\"lowercase:\", word.lower(), end=' | ')\n",
    "    \n",
    "    # characters in strings can be indexed just like items in lists\n",
    "    print(\"first letter:\", word[0].upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KB91dSXxDmWb"
   },
   "source": [
    "- How about dealing with multiple sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rhOzBcRb_zcg"
   },
   "outputs": [],
   "source": [
    "# From https://en.wikipedia.org/wiki/Data_science\n",
    "\n",
    "text =  'Data science is a \"concept to unify statistics, data analysis, machine ' + \\\n",
    "        'learning and their related methods\" in order to \"understand and analyze ' + \\\n",
    "        'actual phenomena\" with data. '\n",
    "\n",
    "text += 'It employs techniques and theories drawn from many fields within the ' + \\\n",
    "        'context of mathematics, statistics, computer science, and information ' + \\\n",
    "        'science. '\n",
    "\n",
    "text += 'Turing award winner Jim Gray imagined data science as a \"fourth paradigm\"' + \\\n",
    "        'of science (empirical, theoretical, computational and now data-driven) ' + \\\n",
    "        'and asserted that \"everything about science is changing because of the ' + \\\n",
    "        'impact of information technology\" and the data deluge. '\n",
    "\n",
    "text += 'In 2015, the American Statistical Association identified database ' + \\\n",
    "        'management, statistics and machine learning, and distributed and ' + \\\n",
    "        'parallel systems as the three emerging foundational professional communities.\"'\n",
    "\n",
    "# We could try splitting on the period character...\n",
    "sentences = text.split('.')\n",
    "print('.\\n'.join(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LpltocCcioIq"
   },
   "source": [
    "- When might this not work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0GsC4WL1Ajkl"
   },
   "outputs": [],
   "source": [
    "# Try this:\n",
    "text =  \"Dr. Martin registered the domain name drmartin.com before moving to the \" + \\\n",
    "        \"U.K. in January. \"\n",
    "text += \"During that time, 1.6 million users visited her website... it was very \" + \\\n",
    "        \"unexpected and caused a server to crash.\"\n",
    "sentences = text.split('.')\n",
    "print('.\\n'.join(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5XUVXkhpivUf"
   },
   "source": [
    "###Introducing the Natural Language Toolkit (NLTK)\n",
    "\n",
    "- NLTK is a very handy library for basic text processing operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-B-ShWH3t0AK"
   },
   "source": [
    "- We can split sentences in a much smarter way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Mh_jMl5i2_0"
   },
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "print('\\n'.join(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uBsjCUlbjIO2"
   },
   "source": [
    "- **What else can we do with NLTK?**\n",
    "- Smarter word tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGuVEhjpENNT"
   },
   "outputs": [],
   "source": [
    "sentence_words = nltk.word_tokenize(sentences[0])\n",
    "print(\"Words:\",' '.join(sentence_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U23FpECZUdNq"
   },
   "source": [
    "- Finding word stems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9i2nyLzCEQ-W"
   },
   "outputs": [],
   "source": [
    "# Add the words from the 2nd sentence\n",
    "sentence_words += nltk.word_tokenize(sentences[1])\n",
    "\n",
    "# Stemming\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stems = [stemmer.stem(word) for word in sentence_words]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8JG_RreJUiDA"
   },
   "source": [
    "Read more about Porter's stemming algorythm [here](https://tartarus.org/martin/PorterStemmer/ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8JG_RreJUiDA"
   },
   "source": [
    "- Labeling words with their part-of-speech, and even finding their lemmas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NHsxvzYET_D"
   },
   "outputs": [],
   "source": [
    "# Part-of-speech tagging\n",
    "pos_tags = nltk.pos_tag(sentence_words)\n",
    "print(\"Parts of speech:\",pos_tags)\n",
    "\n",
    "# Lemmatization\n",
    "def lookup_pos(pos):\n",
    "    pos_first_char = pos[0].lower()\n",
    "    if pos_first_char in 'nv':\n",
    "        return pos_first_char\n",
    "    else:\n",
    "        return 'n'\n",
    "    \n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(word,lookup_pos(pos)) for (word,pos) in pos_tags]\n",
    "print(\"Lemmas:\", ' '.join(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EAlWIzCDUqIb"
   },
   "source": [
    "[Complete list of POS tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EAlWIzCDUqIb"
   },
   "source": [
    "- Sometimes, it is helpful to remove \"stopwords\", like \"a, the, I, do,\" and others.\n",
    "    - It's worth thinking about whether or not these words are important in your application.\n",
    "    - These kinds of words do carry a lot of important information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H95ncKLzEWD7"
   },
   "outputs": [],
   "source": [
    "# Stopword (non-content word) removal\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "content_words = [word for word in sentence_words if word not in stop_words]\n",
    "removed_stop_words = [word for word in sentence_words if word in stop_words]\n",
    "print(\"Content words:\", ' '.join(content_words))\n",
    "print(\"Removed Stop words:\", ' '.join(removed_stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pH6yNFYhVDPq"
   },
   "source": [
    "- Let's look at a simple plot of the word frequencies in our sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OPw6RMjNjmT-"
   },
   "outputs": [],
   "source": [
    "# Get word frequencies\n",
    "frequencies = nltk.probability.FreqDist(sentence_words)\n",
    "\n",
    "# Plot the frequencies\n",
    "frequencies.plot(15,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eLnMR2E1lSea"
   },
   "source": [
    "### Putting it together: Creating a Word Cloud\n",
    "- Now, it's your turn to try out some of the techniques we've covered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oSpw7LyFt5VL"
   },
   "source": [
    "1. First, run the code block below labeled \"Run this code first\" to perform some setup.\n",
    "2. Then, modify the code marked \"Exercise 1\" to convert a document into **preprocessed lemma frequencies**.\n",
    "    - There is a sample solution below. It's hidden for now, but you can take a peek when you are ready.\n",
    "3. Finally, run the code labeled \"build a word cloud\" to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ldc04mMLHXnT"
   },
   "outputs": [],
   "source": [
    "#@title Run this code first: Wordcloud function and loading the document (double-click to view) {display-mode: \"form\"}\n",
    "\n",
    "\n",
    "# Draw a wordcloud!\n",
    "# Inputs:\n",
    "#   word_counts: a dictionary mapping strings to their counts\n",
    "def draw_wordcloud(freq_dist, colormap):\n",
    "    \n",
    "    #TODO add a few corpus specific checks here to make sure people have done casing, lemmatization, punct removal\n",
    "    uniq_count = len(freq_dist.keys())\n",
    "    print(\"Building a word cloud with\",uniq_count,\"unique words...\")\n",
    "    wc = WordCloud(colormap=colormap, width=1500, \n",
    "                   height=1000).generate_from_frequencies(freq_dist)\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "print(\"draw_wordcloud() function is ready to use.\")\n",
    "\n",
    "# Load the contents of the book \"The Wonderful Wizard of Oz\" \n",
    "#   by L. Frank Baum (from project Gutenberg)\n",
    "# document = urllib.request.urlopen(\"http://www.gutenberg.org/cache/epub/55/pg55.txt\").read().decode('utf-8')\n",
    "file_name = \"pg55.txt\"\n",
    "document = open(file_name).read()\n",
    "\n",
    "print('\"The Wonderful Wizard of Oz\" full text is loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hor-Pa-oIgxb"
   },
   "source": [
    "**Exercise 1**\n",
    "\n",
    "Write your code here. Make sure to click the \"run\" button when you're finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yiiymar8Ic33"
   },
   "outputs": [],
   "source": [
    "# Convert text to a dictionary mapping strings to a FreqDist object\n",
    "# containing the frequences of the lemmas in the text.\n",
    "# All stopwords should be removed.\n",
    "# Inputs:\n",
    "#   text: a string as input, possibly containing multiple sentences.\n",
    "def text_to_lemma_frequencies(text):\n",
    "    \n",
    "# ------------- Exercise 1 -------------- #\n",
    "\n",
    "    # write your preprocessing code here\n",
    "\n",
    "    # replace this return function with your own\n",
    "    return nltk.probability.FreqDist([\"Hello\", \"world\", \"hello\", \"world.\"])\n",
    "# ---------------- End ------------------ #\n",
    "\n",
    "    \n",
    "# quick test (do not modify this)\n",
    "test_doc = \"This is a test. Does this work?\"\n",
    "result = text_to_lemma_frequencies(test_doc)\n",
    "passed = result == nltk.probability.FreqDist([\"test\",\"work\"])\n",
    "if passed:\n",
    "    print (\"Test passed!\")\n",
    "else:\n",
    "    print(\"Test did not pass yet.\")\n",
    "    if type(result) == type(nltk.probability.FreqDist([\"a\"])):\n",
    "        print(\"got these words:\", result.keys(),\\\n",
    "              \"\\nwith these counts:\", result.values())\n",
    "    else:\n",
    "        print(\"Did not return a FreqDist object.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJatKHYxR7wt"
   },
   "source": [
    "Now, let's **build a word cloud** for the book \"[The Wonderful Wizard of Oz](http://www.gutenberg.org/cache/epub/55/pg55.txt).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xkWmUoEAlZLn"
   },
   "outputs": [],
   "source": [
    "# Get the word frequency distribution\n",
    "freq_dist = text_to_lemma_frequencies(document)\n",
    "\n",
    "# Use default colormap\n",
    "colormap = None\n",
    "# Bonus: try out some other matplotlib colormaps\n",
    "#colormap = \"spring\" # see more here: https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html\n",
    "\n",
    "# Call the function to draw the word cloud\n",
    "draw_wordcloud(freq_dist, colormap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dED9PJZUp1OR"
   },
   "outputs": [],
   "source": [
    "#@title Sample Solution (double-click to view) Run to load sample solution. {display-mode: \"form\"}\n",
    "\n",
    "def text_to_lemma_frequencies(text, remove_stop_words=True):\n",
    "    \n",
    "    # split document into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # create a place to store (word, pos_tag) tuples\n",
    "    words_and_pos_tags = []\n",
    "    \n",
    "    # get all words and pos tags\n",
    "    for sentence in sentences:\n",
    "        words_and_pos_tags += nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "        \n",
    "    # load the lemmatizer\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # lemmatize the words\n",
    "    lemmas = [lemmatizer.lemmatize(word,lookup_pos(pos)) for \\\n",
    "              (word,pos) in words_and_pos_tags]\n",
    "    \n",
    "    # convert to lowercase\n",
    "    lowercase_lemmas = [lemma.lower() for lemma in lemmas]\n",
    "    \n",
    "    # load the stopword list for English\n",
    "    stop_words = set([])\n",
    "    if remove_stop_words:\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    # add punctuation to the set of things to remove\n",
    "    all_removal_tokens = stop_words | set(string.punctuation)\n",
    "    \n",
    "    # bonus: also add some custom double-quote tokens to this set\n",
    "    all_removal_tokens |= set([\"''\",\"``\"])\n",
    "    \n",
    "    # only get lemmas that aren't in these lists\n",
    "    content_lemmas = [lemma for lemma in lowercase_lemmas \\\n",
    "                      if lemma not in all_removal_tokens]\n",
    "    \n",
    "    # return the frequency distribution object\n",
    "    return nltk.probability.FreqDist(content_lemmas)\n",
    "    \n",
    "# Lemmatization -- redefining this here to make\n",
    "# code block more self-contained\n",
    "def lookup_pos(pos):\n",
    "    pos_first_char = pos[0].lower()\n",
    "    if pos_first_char in 'nv':\n",
    "        return pos_first_char\n",
    "    else:\n",
    "        return 'n'\n",
    "    \n",
    "# quick test:\n",
    "test_doc = \"This is a test. Does this work?\"\n",
    "result = text_to_lemma_frequencies(test_doc)\n",
    "passed = result == nltk.probability.FreqDist([\"test\",\"work\"])\n",
    "if passed:\n",
    "    print (\"Test passed!\")\n",
    "else:\n",
    "    print(\"Test did not pass yet.\")\n",
    "    if type(result) == type(nltk.probability.FreqDist([\"a\"])):\n",
    "        print(\"got these words:\", result.keys(),\\\n",
    "              \"\\nwith these counts:\", result.values())\n",
    "    else:\n",
    "        print(\"Did not return a FreqDist object.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gwsc84RSTT-D"
   },
   "source": [
    "### Bonus: Zipf's Law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "liWR6QyiuC4j"
   },
   "source": [
    "- Let's check the frequency distribution over the top N words in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tDzA0LpYE8P1"
   },
   "outputs": [],
   "source": [
    "top_n_words = 100\n",
    "freq_dist.plot(top_n_words, cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gWEe1E68PelP"
   },
   "source": [
    "- You've just observed (a \"Wizard of Oz\" version of) [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law)  at work!\n",
    "\n",
    "- Remember that we've also removed stopwords. \n",
    "\n",
    "- _Try this_: \n",
    "    - Load the sample `text_to_lemma_frequencies()` function, then run the code below to see what this looks like with stopwords.\n",
    "\n",
    "    - Pay attention to how the y-axis is different from the example above.\n",
    "\n",
    "    - Compare the result to [this example](https://phys.org/news/2017-08-unzipping-zipf-law-solution-century-old.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lnxAMXf9SkQG"
   },
   "outputs": [],
   "source": [
    "freq_dist = text_to_lemma_frequencies(document, remove_stop_words=False)\n",
    "top_n_words = 100\n",
    "freq_dist.plot(top_n_words, cumulative=False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "01_Text_Processsing_Basics.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
