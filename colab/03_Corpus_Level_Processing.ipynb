{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gordeli/NLP_EDHEC/blob/main/colab/03_Corpus_Level_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "APfI_c8B40Vn"
   },
   "source": [
    "#Natural Language Processing @ EDHEC, 2022\n",
    "\n",
    "# Part 3: Corpus Level Processing\n",
    "\n",
    "[<- Previous: Data Collection](https://colab.research.google.com/github/gordeli/NLP_EDHEC/blob/main/colab/02_Data_Collection.ipynb)\n",
    "\n",
    "[-> Next: Content Analysis](https://colab.research.google.com/github/gordeli/NLP_EDHEC/blob/main/colab/04_Content_Analysis.ipynb)\n",
    "\n",
    "Dates: January 31 - February 11, 2022\n",
    "\n",
    "Facilitator: [Ivan Gordeliy](https://www.linkedin.com/in/gordeli/)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cdTajgZhkGWX"
   },
   "source": [
    "## Initial Setup\n",
    "\n",
    "- **Run \"Setup\" below first.**\n",
    "\n",
    "    - This will load libraries and download some resources that we'll use throughout the tutorial.\n",
    "\n",
    "    - You will see a message reading \"Done with setup!\" when this process completes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GKVEnPi34qj4"
   },
   "outputs": [],
   "source": [
    "#@title Setup (click the \"run\" button to the left) {display-mode: \"form\"}\n",
    "\n",
    "## Setup ##\n",
    "\n",
    "# imports\n",
    "\n",
    "# built-in Python libraries\n",
    "# -------------------------\n",
    "\n",
    "# counting and data management\n",
    "import collections\n",
    "# operating system utils\n",
    "import os\n",
    "# regular expressions\n",
    "import re\n",
    "# additional string functions\n",
    "import string\n",
    "# system utilities\n",
    "import sys\n",
    "# request() will be used to load web content\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "# 3rd party libraries\n",
    "# -------------------\n",
    "\n",
    "# Natural Language Toolkit (https://www.nltk.org/)\n",
    "import nltk\n",
    "\n",
    "# download punctuation related NLTK functions\n",
    "# (needed for sent_tokenize())\n",
    "nltk.download('punkt')\n",
    "# download NLKT part-of-speech tagger\n",
    "# (needed for pos_tag())\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# download wordnet\n",
    "# (needed for lemmatization)\n",
    "nltk.download('wordnet')\n",
    "# download stopword lists\n",
    "# (needed for stopword removal)\n",
    "nltk.download('stopwords')\n",
    "# dictionary of English words\n",
    "nltk.download('words')\n",
    "\n",
    "# numpy: matrix library for Python\n",
    "import numpy as np\n",
    "\n",
    "# scipy: scientific operations\n",
    "# works with numpy objects\n",
    "import scipy\n",
    "\n",
    "# matplotlib (and pyplot) for visualizations\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn for basic machine learning operations\n",
    "import sklearn\n",
    "import sklearn.manifold\n",
    "import sklearn.cluster\n",
    "\n",
    "# worldcloud tool\n",
    "!pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# for checking object memory usage\n",
    "!pip install pympler\n",
    "from pympler import asizeof\n",
    "\n",
    "!pip install spacy\n",
    "import spacy\n",
    "\n",
    "# Downloading data\n",
    "# ----------------\n",
    "if not os.path.exists(\"aclImdb\"):\n",
    "    !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    !tar -xzf aclImdb_v1.tar.gz\n",
    "\n",
    "print()\n",
    "print(\"Done with setup!\")\n",
    "print(\"If you'd like, you can click the (X) button to the left to clear this output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4sK9riH96uta"
   },
   "source": [
    "---\n",
    "## A - Recap of [Tutorial 1](https://colab.research.google.com/github/gordeli/NLP_EDHEC/blob/main/colab/01_Text_Processing_Basics.ipynb) material\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UepTkw-gjzAn"
   },
   "source": [
    "\n",
    "### Built-in Python functions\n",
    "\n",
    "- Basic Python fuctions provide a good starting place. # Though it is given only as an example. Use the tokenizaton functions from NLTK instead!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YHQ2OyBNtkf7"
   },
   "source": [
    "- First, we should try to split a sentence into individual words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MWkjtSPv8H7s"
   },
   "outputs": [],
   "source": [
    "text = \"EDHEC Business School (French: Ecole des Hautes Etudes Commerciales du Nord) is a French business school. As a Grande Ã©cole in France, it specializes in business and management studies. It has five campuses: Lille, Nice, Paris, London, and Singapore,[4] and offers undergraduate (BBA), graduate (MSc and MiM), executive education (Global MBA, EMBA) PhD in Finance, and a variety of open and customized programmes.[5] It has 8,000 students enrolled in traditional graduate and undergraduate programmes, 150 partner universities and a network of more than 40,000 alumni in over 125 countries.\"\n",
    "\n",
    "# We can split on all whitespace with split()\n",
    "words = text.split()\n",
    "print(\"WORDS:\",words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5XUVXkhpivUf"
   },
   "source": [
    "###Introducing the Natural Language Toolkit (NLTK)\n",
    "\n",
    "- NLTK is a very handy library for basic text processing operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-B-ShWH3t0AK"
   },
   "source": [
    "- We can split sentences in a much smarter way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Mh_jMl5i2_0"
   },
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "print('\\n'.join(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uBsjCUlbjIO2"
   },
   "source": [
    "- **What else can we do with NLTK?**\n",
    "- Smarter word tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGuVEhjpENNT"
   },
   "outputs": [],
   "source": [
    "sentence_words = nltk.word_tokenize(sentences[0])\n",
    "print(\"Words:\",' '.join(sentence_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U23FpECZUdNq"
   },
   "source": [
    "- Finding word stems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9i2nyLzCEQ-W"
   },
   "outputs": [],
   "source": [
    "# Add the words from the 2nd sentence\n",
    "sentence_words += nltk.word_tokenize(sentences[1])\n",
    "\n",
    "# Stemming\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stems = [stemmer.stem(word) for word in sentence_words]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8JG_RreJUiDA"
   },
   "source": [
    "- Labeling words with their part-of-speech, and even finding their lemmas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NHsxvzYET_D"
   },
   "outputs": [],
   "source": [
    "# Part-of-speech tagging\n",
    "pos_tags = nltk.pos_tag(sentence_words)\n",
    "print(\"Parts of speech:\",pos_tags)\n",
    "\n",
    "# Lemmatization\n",
    "def lookup_pos(pos):\n",
    "    pos_first_char = pos[0].lower()\n",
    "    if pos_first_char in 'nv':\n",
    "        return pos_first_char\n",
    "    else:\n",
    "        return 'n'\n",
    "    \n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(word,lookup_pos(pos)) for (word,pos) in pos_tags]\n",
    "print(\"Lemmas:\", ' '.join(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EAlWIzCDUqIb"
   },
   "source": [
    "- Sometimes, it is helpful to remove \"stopwords\", like \"a, the, I, do,\" and others.\n",
    "    - It's worth thinking about whether or not these words are important in your application.\n",
    "    - These kinds of words do carry a lot of important information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H95ncKLzEWD7"
   },
   "outputs": [],
   "source": [
    "# Stopword (non-content word) removal\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "content_words = [word for word in sentence_words if word not in stop_words]\n",
    "removed_stop_words = [word for word in sentence_words if word in stop_words]\n",
    "print(\"Content words:\", ' '.join(content_words))\n",
    "print(\"Removed Stop words:\", ' '.join(removed_stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pH6yNFYhVDPq"
   },
   "source": [
    "- Let's look at a simple plot of the word frequencies in our sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OPw6RMjNjmT-"
   },
   "outputs": [],
   "source": [
    "# Get word frequencies\n",
    "frequencies = nltk.probability.FreqDist(sentence_words)\n",
    "\n",
    "# Plot the frequencies\n",
    "frequencies.plot(15,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eLnMR2E1lSea"
   },
   "source": [
    "### Putting it together: Creating a Word Cloud\n",
    "- Now, it's your turn to try out some of the techniques we've covered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oSpw7LyFt5VL"
   },
   "source": [
    "1. First, run the code block below labeled \"Run this code first\" to perform some setup.\n",
    "2. Then, modify the code marked \"Exercise 1\" to convert a document into **preprocessed lemma frequencies**.\n",
    "    - There is a sample solution below. It's hidden for now, but you can take a peek when you are ready.\n",
    "3. Finally, run the code labeled \"build a word cloud\" to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ldc04mMLHXnT"
   },
   "outputs": [],
   "source": [
    "#@title Run this code first: Wordcloud function and loading the document (double-click to view) {display-mode: \"form\"}\n",
    "\n",
    "\n",
    "# Draw a wordcloud!\n",
    "# Inputs:\n",
    "#   word_counts: a dictionary mapping strings to their counts\n",
    "def draw_wordcloud(freq_dist, colormap):\n",
    "    \n",
    "    #TODO add a few corpus specific checks here to make sure people have done casing, lemmatization, punct removal\n",
    "    uniq_count = len(freq_dist.keys())\n",
    "    print(\"Building a word cloud with\",uniq_count,\"unique words...\")\n",
    "    wc = WordCloud(colormap=colormap, width=1500, \n",
    "                   height=1000).generate_from_frequencies(freq_dist)\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "print(\"draw_wordcloud() function is ready to use.\")\n",
    "\n",
    "# Load the contents of the book \"The Wonderful Wizard of Oz\" \n",
    "#   by L. Frank Baum (from project Gutenberg)\n",
    "document = urllib.request.urlopen(\"http://www.gutenberg.org/cache/epub/55/pg55.txt\").read().decode('utf-8')\n",
    "# file_name = \"pg55.txt\"\n",
    "# document = open(file_name).read()\n",
    "\n",
    "print('\"The Wonderful Wizard of Oz\" full text is loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hor-Pa-oIgxb"
   },
   "source": [
    "**Exercise 1**\n",
    "\n",
    "Write your code here. Make sure to click the \"run\" button when you're finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yiiymar8Ic33"
   },
   "outputs": [],
   "source": [
    "# Convert text to a dictionary mapping strings to a FreqDist object\n",
    "# containing the frequences of the lemmas in the text.\n",
    "# All stopwords should be removed.\n",
    "# Inputs:\n",
    "#   text: a string as input, possibly containing multiple sentences.\n",
    "def text_to_lemma_frequencies(text):\n",
    "    \n",
    "# ------------- Exercise 1 -------------- #\n",
    "\n",
    "    # write your preprocessing code here\n",
    "\n",
    "    # replace this return function with your own\n",
    "    return nltk.probability.FreqDist([\"Hello\", \"world\", \"hello\", \"world.\"])\n",
    "# ---------------- End ------------------ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dED9PJZUp1OR"
   },
   "outputs": [],
   "source": [
    "#@title Sample Solution (double-click to view) Run to load sample solution. {display-mode: \"form\"}\n",
    "\n",
    "def text_to_lemma_frequencies(text, remove_stop_words=True):\n",
    "    \n",
    "    # split document into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # create a place to store (word, pos_tag) tuples\n",
    "    words_and_pos_tags = []\n",
    "    \n",
    "    # get all words and pos tags\n",
    "    for sentence in sentences:\n",
    "        words_and_pos_tags += nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "        \n",
    "    # load the lemmatizer\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # lemmatize the words\n",
    "    lemmas = [lemmatizer.lemmatize(word,lookup_pos(pos)) for \\\n",
    "              (word,pos) in words_and_pos_tags]\n",
    "    \n",
    "    # convert to lowercase\n",
    "    lowercase_lemmas = [lemma.lower() for lemma in lemmas]\n",
    "    \n",
    "    # load the stopword list for English\n",
    "    stop_words = set([])\n",
    "    if remove_stop_words:\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    # add punctuation to the set of things to remove\n",
    "    all_removal_tokens = stop_words | set(string.punctuation)\n",
    "    \n",
    "    # bonus: also add some custom double-quote tokens to this set\n",
    "    all_removal_tokens |= set([\"''\",\"``\"])\n",
    "    \n",
    "    # only get lemmas that aren't in these lists\n",
    "    content_lemmas = [lemma for lemma in lowercase_lemmas \\\n",
    "                      if lemma not in all_removal_tokens]\n",
    "    \n",
    "    # return the frequency distribution object\n",
    "    return nltk.probability.FreqDist(content_lemmas)\n",
    "    \n",
    "# Lemmatization -- redefining this here to make\n",
    "# code block more self-contained\n",
    "def lookup_pos(pos):\n",
    "    pos_first_char = pos[0].lower()\n",
    "    if pos_first_char in 'nv':\n",
    "        return pos_first_char\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJatKHYxR7wt"
   },
   "source": [
    "Now, let's **build a word cloud** for the book \"[The Wonderful Wizard of Oz](http://www.gutenberg.org/cache/epub/55/pg55.txt).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xkWmUoEAlZLn"
   },
   "outputs": [],
   "source": [
    "# Get the word frequency distribution\n",
    "freq_dist = text_to_lemma_frequencies(document)\n",
    "\n",
    "# Use default colormap\n",
    "colormap = None\n",
    "# Bonus: try out some other matplotlib colormaps\n",
    "#colormap = \"spring\" # see more here: https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html\n",
    "\n",
    "# Call the function to draw the word cloud\n",
    "draw_wordcloud(freq_dist, colormap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gwsc84RSTT-D"
   },
   "source": [
    "### Bonus: Zipf's Law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "liWR6QyiuC4j"
   },
   "source": [
    "- Let's check the frequency distribution over the top N words in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tDzA0LpYE8P1"
   },
   "outputs": [],
   "source": [
    "top_n_words = 100\n",
    "freq_dist.plot(top_n_words, cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gWEe1E68PelP"
   },
   "source": [
    "- You've just observed (a \"Wizard of Oz\" version of) [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law)  at work!\n",
    "\n",
    "- Remember that we've also removed stopwords. \n",
    "\n",
    "- _Try this_: \n",
    "    - Load the sample `text_to_lemma_frequencies()` function, then run the code below to see what this looks like with stopwords.\n",
    "\n",
    "    - Pay attention to how the y-axis is different from the example above.\n",
    "\n",
    "    - Compare the result to [this example](https://phys.org/news/2017-08-unzipping-zipf-law-solution-century-old.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lnxAMXf9SkQG"
   },
   "outputs": [],
   "source": [
    "freq_dist = text_to_lemma_frequencies(document, remove_stop_words=False)\n",
    "top_n_words = 100\n",
    "freq_dist.plot(top_n_words, cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eRm3uG2Gt9oZ"
   },
   "source": [
    "---\n",
    "## B - Corpus-level Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KHJ8xHQRTO1a"
   },
   "outputs": [],
   "source": [
    "#@title Skipped part A? Run this cell to load code needed moving forward. {display-mode: \"form\"}\n",
    "\n",
    "print(\"Make sure that you have run 'Initial Setup'!\")\n",
    "# Setup from part 1\n",
    "\n",
    "def text_to_lemma_frequencies(text, remove_stop_words=True):\n",
    "    \n",
    "    # split document into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # create a place to store (word, pos_tag) tuples\n",
    "    words_and_pos_tags = []\n",
    "    \n",
    "    # get all words and pos tags\n",
    "    for sentence in sentences:\n",
    "        words_and_pos_tags += nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "        \n",
    "    # load the lemmatizer\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # lemmatize the words\n",
    "    lemmas = [lemmatizer.lemmatize(word,lookup_pos(pos)) for \\\n",
    "              (word,pos) in words_and_pos_tags]\n",
    "    \n",
    "    # convert to lowercase\n",
    "    lowercase_lemmas = [lemma.lower() for lemma in lemmas]\n",
    "    \n",
    "    # load the stopword list for English\n",
    "    stop_words = set([])\n",
    "    if remove_stop_words:\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    # add punctuation to the set of things to remove\n",
    "    all_removal_tokens = stop_words | set(string.punctuation)\n",
    "    \n",
    "    # bonus: also add some custom double-quote tokens to this set\n",
    "    all_removal_tokens |= set([\"''\",\"``\"])\n",
    "    \n",
    "    # only get lemmas that aren't in these lists\n",
    "    content_lemmas = [lemma for lemma in lowercase_lemmas \\\n",
    "                      if lemma not in all_removal_tokens]\n",
    "    \n",
    "    # return the frequency distribution object\n",
    "    return nltk.probability.FreqDist(content_lemmas)\n",
    "    \n",
    "# Lemmatization -- redefining this here to make\n",
    "# code block more self-contained\n",
    "def lookup_pos(pos):\n",
    "    pos_first_char = pos[0].lower()\n",
    "    if pos_first_char in 'nv':\n",
    "        return pos_first_char\n",
    "    else:\n",
    "        return 'n'\n",
    "    \n",
    "print(\"Otherwise, you're now ready for part 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qEaT-4UAwXCk"
   },
   "source": [
    "### Matrix Representations\n",
    "\n",
    "- Representing documents as vectors of words gets us one step closer to using traditional data science approaches.\n",
    "\n",
    "- However, never forget that we're still working with language data!\n",
    "\n",
    "- **How do we get a corpus matrix?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JtgHwyi4uHi3"
   },
   "source": [
    "\n",
    "- First, we'll load a small corpus into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VvNorDdMxQ8Y"
   },
   "outputs": [],
   "source": [
    "# from the Stanford Movie Reviews Data: \n",
    "# http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "# we downloaded this during our initial Setup\n",
    "movie_review_dir = \"aclImdb/train/unsup/\"\n",
    "movie_review_files = os.listdir(movie_review_dir)\n",
    "n_movie_reviews = []\n",
    "n = 50\n",
    "for txt_file_path in sorted(movie_review_files, \\\n",
    "                            key=lambda x:int(x.split('_')[0]))[:n]:\n",
    "        full_path = movie_review_dir + txt_file_path\n",
    "        with open(full_path,'r') as txt_file:\n",
    "            n_movie_reviews.append(txt_file.read())\n",
    "            \n",
    "print(\"Loaded\",len(n_movie_reviews),\"movie reviews from the Stanford IMDB \" + \\\n",
    "      \"corpus into memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vSzT0qoQxhmm"
   },
   "source": [
    "- Start by getting a bag-of-words representation for each review.\n",
    "- Then, create a mapping between the full vocabulary and columns for our matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ap8M_6mBwV5f"
   },
   "outputs": [],
   "source": [
    "review_frequency_distributions = []\n",
    "\n",
    "# process each review, one at a time\n",
    "for review in n_movie_reviews:\n",
    "    \n",
    "    # let's use our function from before\n",
    "    frequencies = text_to_lemma_frequencies(review)\n",
    "    review_frequency_distributions.append(frequencies)\n",
    "\n",
    "# use a dictionary for faster lookup\n",
    "vocab2index = {}\n",
    "latest_index = 0\n",
    "for rfd in review_frequency_distributions:\n",
    "    for token in rfd.keys():\n",
    "        if token not in vocab2index:\n",
    "            vocab2index[token] = latest_index\n",
    "            latest_index += 1\n",
    "    \n",
    "print(\"Built vocab lookup for vocab of size:\",len(vocab2index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u0NkAsWbu0Do"
   },
   "source": [
    "- Given the frequencies and this index lookup, we can build a frequency matrix (as a numpy array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6NUN92WVu8zU"
   },
   "outputs": [],
   "source": [
    "# make an all-zero numpy array with shape n x v\n",
    "# n = number of documents\n",
    "# v = vocabulary size\n",
    "corpus_matrix = np.zeros((len(review_frequency_distributions), len(vocab2index)))\n",
    "\n",
    "# fill in the numpy array\n",
    "for row, rfd in enumerate(review_frequency_distributions):\n",
    "    for token, frequency in rfd.items():\n",
    "        column = vocab2index[token]\n",
    "        corpus_matrix[row][column] = frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MoZ7HKOKzRBX"
   },
   "outputs": [],
   "source": [
    "# get some basic information about our matrix\n",
    "def print_matrix_info(m):\n",
    "    print(\"Our corpus matrix is\",m.shape[0],'x',m.shape[1])\n",
    "    print(\"Sparsity is:\",round(float(100 * np.count_nonzero(m))/ \\\n",
    "                           (m.shape[0] * m.shape[1]),2),\"%\")\n",
    "\n",
    "print_matrix_info(corpus_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "koO0_3BSxg_p"
   },
   "source": [
    "- Now that we've seen how this works, let's see how some existing Python functions can do the heavy lifting for us.\n",
    "- Scikit learn has some useful feature extraction methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FpsI7crJxnPu"
   },
   "outputs": [],
   "source": [
    "# we can get a similar corpus matrix with just 3 lines of code\n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer()\n",
    "sklearn_corpus_data = vectorizer.fit_transform(n_movie_reviews)\n",
    "sklearn_corpus_matrix = sklearn_corpus_data.toarray()\n",
    "\n",
    "# get the feature names (1:1 mapping to the columns in the matrix)\n",
    "print(\"First 10 features:\",vectorizer.get_feature_names_out()[:10])\n",
    "print()\n",
    "\n",
    "# let's check out the matrix\n",
    "print_matrix_info(sklearn_corpus_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cphShoYmxvN1"
   },
   "source": [
    "- These matrices are typically _very_ sparse.\n",
    "- It's worth considering [different representations](https://docs.scipy.org/doc/scipy/reference/sparse.html) if memory is a concern.\n",
    "    - Save space by only storing nonzero entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RGD18Za9x1Gm"
   },
   "outputs": [],
   "source": [
    "# E.g., using a CSR matrix representation\n",
    "csr_corpus_matrix = scipy.sparse.csr_matrix(corpus_matrix)\n",
    "print(\"Original matrix: using\", asizeof.asizeof(corpus_matrix)/1000,\"kB\")\n",
    "print(\"CSR matrix: using\", asizeof.asizeof(csr_corpus_matrix)/1000,\"kB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6F8bb6cPuGMM"
   },
   "source": [
    "- There will be a trade-off between memory usage and speed of operations.\n",
    "    - consider the strengths and weaknesses of each representation.\n",
    "        - e.g., CSR has fast row-level operations, but slow column-level operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8WeZEZ5pyO5I"
   },
   "source": [
    "### Document Retrieval and Similarity\n",
    "\n",
    "- With this matrix, it's very easy to find all documents containing a specific word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lRJYmn4WylNW"
   },
   "outputs": [],
   "source": [
    "search_term = \"funny\"\n",
    "if search_term in vocab2index:\n",
    "    search_index = vocab2index[search_term]\n",
    "    matches = [i for i in range(corpus_matrix.shape[0]) \\\n",
    "           if corpus_matrix[i][search_index]!=0]\n",
    "\n",
    "    # list the documents that contain the search term\n",
    "    print(\"These documents contain '\"+search_term+\"':\",matches)\n",
    "    print()\n",
    "\n",
    "    # show excerpt where this word appears\n",
    "    example_location = n_movie_reviews[matches[0]].find(search_term)\n",
    "    start,end = max(example_location-30,0), min(example_location+30,len(n_movie_reviews[matches[0]]))\n",
    "    print('For example: \"...',n_movie_reviews[matches[0]][start:end],'...\"')\n",
    "    \n",
    "else:\n",
    "    print(search_term,\"isn't in the sample corups.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Ar0P-szymQg"
   },
   "source": [
    "- We can even use the notion of vector representations to compute the similarity between two documents.\n",
    "\n",
    "    - (we'll talk about more advanced ways to approach this task later in the tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wRGicr44CIGC"
   },
   "outputs": [],
   "source": [
    "example_docs =[ \"My dog likes to eat vegetables\",\\\n",
    "                \"Your dog likes to eat fruit\",\\\n",
    "                \"The computer is offline\",\\\n",
    "                \"A computer shouldn't be offline\" ]\n",
    "\n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer()\n",
    "example_data = vectorizer.fit_transform(example_docs)\n",
    "example_matrix = example_data.toarray()\n",
    "\n",
    "sim_0_1 = 1-scipy.spatial.distance.cosine(example_matrix[0],example_matrix[1])\n",
    "sim_2_3 = 1-scipy.spatial.distance.cosine(example_matrix[2],example_matrix[3])\n",
    "sim_0_2 = 1-scipy.spatial.distance.cosine(example_matrix[0],example_matrix[2])\n",
    "\n",
    "print(\"Similarity between 0 and 1:\",round(sim_0_1,2))\n",
    "print(\"Similarity between 2 and 3:\",round(sim_2_3,2))\n",
    "print(\"Similarity between 0 and 2:\",round(sim_0_2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYG5eIG7CKqZ"
   },
   "source": [
    "- We can do the same thing with our corpus of movie reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x0QOEBZVy0ME"
   },
   "outputs": [],
   "source": [
    "# choose a document, and find the most \"similar\" other document in the corpus\n",
    "reference_doc = 0\n",
    "ref_doc_vec = corpus_matrix[reference_doc]\n",
    "sim_to_ref_doc = []\n",
    "for row in corpus_matrix:\n",
    "    sim_to_ref_doc.append(1-scipy.spatial.distance.cosine(ref_doc_vec,row))\n",
    "    \n",
    "print(\"similarity scores:\",sim_to_ref_doc)\n",
    "most_similar = sim_to_ref_doc.index(max(sim_to_ref_doc[1:]))\n",
    "print(n_movie_reviews[0])\n",
    "print(\"is most similar to\")\n",
    "print(n_movie_reviews[most_similar])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-aWERohPy7wA"
   },
   "source": [
    "### Putting it together: Simple Document Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aFm9jdcYL_gn"
   },
   "source": [
    "- Let's apply the document to matrix idea to do some simple clustering.\n",
    "- First, let's load a dataset that should exhibit some natural groupings based on topic.\n",
    "    - [20news](http://qwone.com/~jason/20Newsgroups/) is classic NLP dataset for document classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r9otrBrGIPE-"
   },
   "outputs": [],
   "source": [
    "# load 20 newsgroups dataset - just 100 texts from 3 categories\n",
    "categories = ['comp.sys.ibm.pc.hardware', 'rec.sport.baseball']\n",
    "newsgroups_train_all = sklearn.datasets.fetch_20newsgroups(subset='train',\\\n",
    "                                                 categories=categories)\n",
    "newsgroups_train = newsgroups_train_all.data[:100]\n",
    "newsgroups_labels = newsgroups_train_all.target[:100]\n",
    "\n",
    "print(\"Loaded\",len(newsgroups_train),\"documents.\")\n",
    "print(\"Label distribution:\",collections.Counter(newsgroups_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kJWL6_y4MZzO"
   },
   "source": [
    "**Exercise 2**\n",
    "\n",
    "- Now, write a function that creates a corpus matrix from a list of strings containing documents.\n",
    "    - We can use the `text_to_lemma_frequencies` that you wrote earlier as a starting point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SkQOyk23Ll1Z"
   },
   "outputs": [],
   "source": [
    "# ------------- Exercise 2 -------------- #\n",
    "def docs2matrix(document_list):\n",
    "    \n",
    "    # this should be a nice starting point\n",
    "    lemma_freqs = [text_to_lemma_frequencies(doc) for doc in document_list]\n",
    "\n",
    "    # change this to return a 2d numpy array\n",
    "    return None\n",
    "\n",
    "# -------------     End    -------------- #\n",
    "\n",
    "# quick test with first 10 documents\n",
    "X = docs2matrix(newsgroups_train[:10])\n",
    "if type(X) != type(np.zeros([3,3])):\n",
    "    print(\"Did not return a 2d numpy matrix.\")\n",
    "elif X.shape[0] != 10:\n",
    "    print(\"number of rows should be 10, but is\",X.shape[0])\n",
    "else:\n",
    "    print(\"Created a matrix with shape:\",X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJy4zWGT4uHi"
   },
   "outputs": [],
   "source": [
    "#@title Sample Solution (double-click to view) {display-mode: \"form\"}\n",
    "\n",
    "def docs2matrix(document_list):\n",
    "    \n",
    "    # use the vocab2index idea from before\n",
    "    vocab2index = {}\n",
    "    \n",
    "    # this should be a nice starting point\n",
    "    lemma_freqs = [text_to_lemma_frequencies(doc) for doc in document_list]\n",
    "\n",
    "    latest_index = 0\n",
    "    for lf in lemma_freqs:\n",
    "        for token in lf.keys():\n",
    "            if token not in vocab2index:\n",
    "                vocab2index[token] = latest_index\n",
    "                latest_index += 1\n",
    "    \n",
    "    # create the zeros matrix\n",
    "    corpus_matrix = np.zeros((len(lemma_freqs), len(vocab2index)))\n",
    "    \n",
    "    for row, lf in enumerate(lemma_freqs):\n",
    "        for token, frequency in lf.items():\n",
    "            column = vocab2index[token]\n",
    "            corpus_matrix[row][column] = frequency\n",
    "    \n",
    "    # change this to return a 2d numpy array\n",
    "    return corpus_matrix\n",
    "\n",
    "\n",
    "# quick test with first 10 documents\n",
    "X = docs2matrix(newsgroups_train[:10])\n",
    "if type(X) != type(np.zeros([3,3])):\n",
    "    print(\"Did not return a 2d numpy matrix.\")\n",
    "elif X.shape[0] != 10:\n",
    "    print(\"number of rows should be 10, but is\",X.shape[0])\n",
    "else:\n",
    "    print(\"Created a matrix with shape:\",X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QVt7UvF88njv"
   },
   "source": [
    "- Let's visualize the data in 2 dimensions\n",
    "    - We'll use [T-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to do the dimensionality reduction.\n",
    "    - Each color (red and blue) will represent one of the \"groun truth\" clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cegveYxELYR6"
   },
   "outputs": [],
   "source": [
    "# show corpus in 2d\n",
    "\n",
    "X = docs2matrix(newsgroups_train)\n",
    "print(\"Created a matrix with shape:\",X.shape)\n",
    "tsne = sklearn.manifold.TSNE(n_components=2, random_state=1)\n",
    "X_2d = tsne.fit_transform(X)\n",
    "colors = ['r', 'b']\n",
    "target_ids = range(len(categories))\n",
    "for target, c, label in zip(target_ids, colors, categories):\n",
    "    plt.scatter(X_2d[newsgroups_labels == target, 0], X_2d[newsgroups_labels == target, 1], c=c, label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CoXfEBDndKG-"
   },
   "source": [
    "- The groups have a fair degree of overlap. Can kmeans clustering recover them correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WIaFvS1-dYM9"
   },
   "outputs": [],
   "source": [
    "# Do kmeans clustering\n",
    "\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters=2, random_state=0, algorithm=\"full\").fit(X)\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "for target, c, label in zip(target_ids, colors, categories):\n",
    "    plt.scatter(X_2d[clusters == target, 0], X_2d[clusters == target, 1], c=c, label=label)\n",
    "\n",
    "# out own purity function\n",
    "def compute_average_purity(clusters, labels):\n",
    "    # and computer the cluster purity\n",
    "    cluster_labels = collections.defaultdict(list)\n",
    "    for i in range(len(clusters)):\n",
    "        cluster = clusters[i]\n",
    "        label = labels[i]\n",
    "        cluster_labels[cluster].append(label)\n",
    "    cluster_purities = {}\n",
    "    for cluster, labels in cluster_labels.items():\n",
    "        most_common_count = collections.Counter(labels).most_common()[0][1]\n",
    "        purity = float(most_common_count)/len(labels)\n",
    "        cluster_purities[cluster] = purity\n",
    "    avg_purity = sum(cluster_purities.values())/len(cluster_purities.keys())\n",
    "    print(\"Average cluster purity:\",avg_purity)\n",
    "    \n",
    "avg_purity = compute_average_purity(clusters, newsgroups_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-FEqeMWSdYuS"
   },
   "source": [
    "- That didn't work as well as we'd like it to.\n",
    "- It's time to introduce better features that just word frequencies.\n",
    "    - TF-IDF to the rescue!\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WWWn4Re11dBj"
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YLnoPCKVAvrO"
   },
   "source": [
    "- Some words are less important when making distinctions between documents in a corpus.\n",
    "- How can we determine the \"less important\" words?\n",
    "    - Using term-frequency * inverse document frequency, we make the assumption that words that appear in *many documents* are *less informative* overall.\n",
    "    - Therefore, we weigh each term based on the inverse of the number of documents that that term appears in.\n",
    "    - We can define $\\operatorname{tfidf}(t,d,D) = \\operatorname{tf}(t,d) * \\log\\frac{|D|}{|d \\in D : t \\in d|}$ , where\n",
    "        - $t$ is a term (token) in a corpus\n",
    "        - $d$ is a document in the corpus\n",
    "        - $D$ is the corpus itself, containing documents, which, in turn, contain tokens\n",
    "        - $\\operatorname{tf}(t,d)$ is the frequency of $t$ in $d$ (typically normalized at the document level).\n",
    "- sklearn has another vectorizer that takes care of this for us: the [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "    - It behaves just like the CountVectorizer() that we saw before, except it computes tfidf scores instead of counts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XFcY3Gm-JT41"
   },
   "source": [
    "- Of course we can just use the TfidfVectorizer, but what would it look like to implement this ourselves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_KSXAxnL09Dq"
   },
   "outputs": [],
   "source": [
    "# assume input matrix contains term frequencies\n",
    "def tfidf_transform(mat):\n",
    "    \n",
    "    # convert matrix of counts to matrix of normalized frequencies\n",
    "    normalized_mat = mat / np.transpose(mat.sum(axis=1)[np.newaxis])\n",
    "    \n",
    "    # compute IDF scores for each word given the corpus\n",
    "    docs_using_terms = np.count_nonzero(mat,axis=0)\n",
    "    idf_scores = np.log(mat.shape[1]/docs_using_terms)\n",
    "    \n",
    "    # compuite tfidf scores\n",
    "    tfidf_mat = normalized_mat * idf_scores\n",
    "    return tfidf_mat\n",
    "\n",
    "tfidf_X = tfidf_transform(X)\n",
    "print(\"Counts:\",X[0][0:10])\n",
    "print(\"TFIDF scores:\",tfidf_X[0][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nlbVxUYXJbL0"
   },
   "source": [
    "- What happens if we use tfidf instead of just counts or frequencies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9e7YdnPycz_D"
   },
   "outputs": [],
   "source": [
    "# show corpus in 2d\n",
    "\n",
    "#X = docs2matrix(newsgroups_train)\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(newsgroups_train).todense()\n",
    "print(\"Created a matrix with shape:\",X.shape)\n",
    "tsne = sklearn.manifold.TSNE(n_components=2, random_state=1)\n",
    "X_2d = tsne.fit_transform(X)\n",
    "colors = ['r', 'b']\n",
    "target_ids = range(len(categories))\n",
    "for target, c, label in zip(target_ids, colors, categories):\n",
    "    plt.scatter(X_2d[newsgroups_labels == target, 0], X_2d[newsgroups_labels == target, 1], c=c, label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "taQcp0UAJh4g"
   },
   "source": [
    "- These groups appear to have a bit more separation.\n",
    "- How well can kmeans recover the original groups now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v4TatTfFzB7K"
   },
   "outputs": [],
   "source": [
    "# Do kmeans clustering with TF-IDF matrisx\n",
    "\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters=2, random_state=0, algorithm=\"full\").fit(X)\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "for target, c, label in zip(target_ids, colors, categories):\n",
    "    plt.scatter(X_2d[clusters == target, 0], X_2d[clusters == target, 1], c=c, label=label)\n",
    "    \n",
    "avg_purity = compute_average_purity(clusters, newsgroups_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gARK1cTYWMWU"
   },
   "source": [
    "### Bonus: SpaCy\n",
    "- If you have extra time, check out the [SpaCy 101 tutorial](https://spacy.io/usage/spacy-101)!\n",
    "    - SpaCy is less research focused, but after you have a good grasp on the core concepts, it can provide a powerful set of NLP tools, and it is definitely worth knowing about.\n",
    "        - It is also often faster to run than NLTK.\n",
    "        - (we will time our nltk version first, for reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HK1p4QRx3b33"
   },
   "outputs": [],
   "source": [
    "%timeit docs2matrix(newsgroups_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yYYxRB_1q9jA"
   },
   "outputs": [],
   "source": [
    "# Example preprocessing with SpaCy\n",
    "def text_to_lemma_frequencies(text):\n",
    "    nlp = spacy.load('en')\n",
    "    doc = nlp(text)\n",
    "    words = [token.lemma for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "    return collections.Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DW4X4ZzwsgFP"
   },
   "outputs": [],
   "source": [
    "# Example document matrix building \n",
    "X = docs2matrix(newsgroups_train)\n",
    "print(\"Created a matrix with shape:\",X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PliAhvcf3Uxl"
   },
   "outputs": [],
   "source": [
    "%timeit docs2matrix(newsgroups_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aC64NI4B4u-M"
   },
   "source": [
    "- Why so slow?\n",
    "    - SpaCy is doing too many tasks that we don't need here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AByJBR2y4225"
   },
   "outputs": [],
   "source": [
    "NLP = spacy.load('en',disable=['ner','parser'])\n",
    "def text_to_lemma_frequencies(text):    \n",
    "    doc = NLP(text)\n",
    "    words = [token.lemma for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "    return collections.Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NzO9KyBr5kgk"
   },
   "outputs": [],
   "source": [
    "%timeit docs2matrix(newsgroups_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "- That's all of the basic text processing that we're going to cover for now.\n",
    "\n",
    "- [-> Next: Content Analysis](https://colab.research.google.com/github/gordeli/NLP_EDHEC/blob/main/04_Content_Analysis.ipynb)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "03_Corpus_Level_Processing.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
