{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gordeli/NLP_EDHEC/blob/main/colab/05_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "APfI_c8B40Vn"
   },
   "source": [
    "#Natural Language Processing @ EDHEC, 2022\n",
    "\n",
    "# Part 5: Final\n",
    "\n",
    "[<- Previous: Content Analysis](https://colab.research.google.com/github/gordeli/NLP_EDHEC/blob/main/colab/04_Content_Analysis.ipynb)\n",
    "\n",
    "Dates: January 31 - February 11, 2022\n",
    "\n",
    "Facilitator: [Ivan Gordeliy](https://www.linkedin.com/in/gordeli/)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cdTajgZhkGWX"
   },
   "source": [
    "## Initial Setup\n",
    "\n",
    "- **Run \"Setup\" below first.**\n",
    "\n",
    "    - This will load libraries and download some resources that we'll use throughout the tutorial.\n",
    "\n",
    "    - You will see a message reading \"Done with setup!\" when this process completes.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code",
    "id": "cdTajgZhkGWX"
   },
   "outputs": [],
   "source": [
    "#@title Setup (click the \"run\" button to the left) {display-mode: \"form\"}\n",
    "\n",
    "## Setup ##\n",
    "\n",
    "# imports\n",
    "\n",
    "# built-in Python libraries\n",
    "# -------------------------\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 3rd party libraries\n",
    "# -------------------\n",
    "\n",
    "# Natural Language Toolkit (https://www.nltk.org/)\n",
    "import nltk\n",
    "\n",
    "# download punctuation related NLTK functions\n",
    "# (needed for sent_tokenize())\n",
    "nltk.download('punkt')\n",
    "# download NLKT part-of-speech tagger\n",
    "# (needed for pos_tag())\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# download wordnet\n",
    "# (needed for lemmatization)\n",
    "nltk.download('wordnet')\n",
    "# download stopword lists\n",
    "# (needed for stopword removal)\n",
    "nltk.download('stopwords')\n",
    "# dictionary of English words\n",
    "nltk.download('words')\n",
    "\n",
    "# numpy: matrix library for Python\n",
    "import numpy as np\n",
    "\n",
    "!pip install -U gensim\n",
    "\n",
    "# Gensim for topic modeling\n",
    "import gensim\n",
    "# for loading data\n",
    "import sklearn.datasets\n",
    "# for LDA visualization\n",
    "!pip install pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "# for uploading data files\n",
    "from google.colab import files\n",
    "\n",
    "# downloading values lexicon\n",
    "!wget https://raw.githubusercontent.com/steve-wilson/values_lexicon/master/lexicon_1_0/values_lexicon.txt\n",
    "!wget https://raw.githubusercontent.com/steve-wilson/values_lexicon/master/sample_data/subreddits/christian_500.txt\n",
    "!wget https://raw.githubusercontent.com/steve-wilson/values_lexicon/master/sample_data/subreddits/business_500.txt\n",
    "!wget https://raw.githubusercontent.com/steve-wilson/values_lexicon/master/sample_data/subreddits/college_500.txt\n",
    "\n",
    "def text_to_lemma_frequencies(text, remove_stop_words=True):\n",
    "    \n",
    "    # split document into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # create a place to store (word, pos_tag) tuples\n",
    "    words_and_pos_tags = []\n",
    "    \n",
    "    # get all words and pos tags\n",
    "    for sentence in sentences:\n",
    "        words_and_pos_tags += nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "        \n",
    "    # load the lemmatizer\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # lemmatize the words\n",
    "    lemmas = [lemmatizer.lemmatize(word,lookup_pos(pos)) for \\\n",
    "              (word,pos) in words_and_pos_tags]\n",
    "    \n",
    "    # convert to lowercase\n",
    "    lowercase_lemmas = [lemma.lower() for lemma in lemmas]\n",
    "    \n",
    "    # load the stopword list for English\n",
    "    stop_words = set([])\n",
    "    if remove_stop_words:\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    # add punctuation to the set of things to remove\n",
    "    all_removal_tokens = stop_words | set(string.punctuation)\n",
    "    \n",
    "    # bonus: also add some custom double-quote tokens to this set\n",
    "    all_removal_tokens |= set([\"''\",\"``\"])\n",
    "    \n",
    "    # only get lemmas that aren't in these lists\n",
    "    content_lemmas = [lemma for lemma in lowercase_lemmas \\\n",
    "                      if lemma not in all_removal_tokens and \\\n",
    "                      re.match(r\"^\\w+$\",lemma)]\n",
    "    \n",
    "    # return the frequency distribution object\n",
    "    return nltk.probability.FreqDist(content_lemmas)\n",
    "    \n",
    "def docs2matrix(document_list):\n",
    "    \n",
    "    # use the vocab2index idea from before\n",
    "    vocab2index = {}\n",
    "    \n",
    "    # load the stopword list for English\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    stop_words |= set(['from', 'subject', 're', 'edu', 'use'])\n",
    "    \n",
    "    # add punctuation to the set of things to remove\n",
    "    all_removal_tokens = stop_words | set(string.punctuation)\n",
    "    \n",
    "    # bonus: also add some custom double-quote tokens to this set\n",
    "    all_removal_tokens |= set([\"''\",\"``\"])\n",
    "    \n",
    "    vocab2index = {}\n",
    "    latest_index = 0\n",
    "\n",
    "    lfs = []\n",
    "    # this should be a nice starting point\n",
    "    for doc in document_list:\n",
    "        lf = text_to_lemma_frequencies(doc,all_removal_tokens)\n",
    "        for token in lf.keys():\n",
    "            if token not in vocab2index:\n",
    "                vocab2index[token] = latest_index\n",
    "                latest_index += 1\n",
    "                \n",
    "        lfs.append(lf)\n",
    "    \n",
    "    # create the zeros matrix\n",
    "    corpus_matrix = np.zeros((len(lfs), len(vocab2index)))\n",
    "    \n",
    "    for row, lf in enumerate(lfs):\n",
    "        for token, frequency in lf.items():\n",
    "            column = vocab2index[token]\n",
    "            corpus_matrix[row][column] = frequency\n",
    "    \n",
    "    return corpus_matrix, vocab2index\n",
    "\n",
    "    \n",
    "# Lemmatization -- redefining this here to make\n",
    "# code block more self-contained\n",
    "def lookup_pos(pos):\n",
    "    pos_first_char = pos[0].lower()\n",
    "    if pos_first_char in 'nv':\n",
    "        return pos_first_char\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "\n",
    "            \n",
    "print()\n",
    "print(\"Done with setup!\")\n",
    "print(\"If you'd like, you can click the (X) button to the left to clear this output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "- Now that we have some real data, what are some ways that we can explore what's in it?\n",
    "    - How can we answer the basic question: *What are people talking about in this corpus?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "- Load a corpus matrix, like the ones we created earlier, into gensim's corpus object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code",
    "id": "mRVVvxfwprwY"
   },
   "outputs": [],
   "source": [
    "# this time, let's load all documents in the 20news dataset from these categories\n",
    "categories = ['soc.religion.christian', 'rec.autos', 'talk.politics.misc', \\\n",
    "              'rec.sport.baseball', 'comp.sys.ibm.pc.hardware']\n",
    "newsgroups_train_all = sklearn.datasets.fetch_20newsgroups(subset='train', \\\n",
    "                                              categories=categories).data\n",
    "# using the function we wrote before, but modified to also return the vocab2index\n",
    "corpus_matrix, word2id = docs2matrix(newsgroups_train_all)\n",
    "# reverse this dictionary\n",
    "id2word = {v:k for k,v in word2id.items()}\n",
    "\n",
    "corpus = gensim.matutils.Dense2Corpus(corpus_matrix, documents_columns=False)\n",
    "print(\"Loaded\",len(corpus),\"documents into a Gensim corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "- Given this, we can run LDA right out of the box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code",
    "id": "mRVVvxfwprwY"
   },
   "outputs": [],
   "source": [
    "# As of July 2019, gensim calls a deprecated numpy function and gives lots of warning messages\n",
    "# Let's supress these.\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# run LDA on our corpus, using out dictionary (k=6)\n",
    "lda = gensim.models.LdaModel(corpus, id2word=id2word, num_topics=6)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "- There is still quite a bit of noise in this list because the documents are full of very common words like \"write\", \"subject\", and \"from\".\n",
    "- One common approach is to remove the most (and possibly least) common words before running LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code",
    "id": "mRVVvxfwprwY"
   },
   "outputs": [],
   "source": [
    "total_counts = np.sum(corpus_matrix, axis=0)\n",
    "sorted_words = sorted( zip( range(len(total_counts)) ,total_counts), \\\n",
    "                       key=lambda x:x[1], reverse=True )\n",
    "N = 100\n",
    "M = 50\n",
    "top_N_ids = [item[0] for item in sorted_words[:N]]\n",
    "appears_less_than_M_times = [item[0] for item in sorted_words if item[1] < M]\n",
    "vocab_dense = [id2word[idx] for idx in range(len(id2word))]\n",
    "\n",
    "print(\"Top words to remove:\", ' '.join([id2word[idx] for idx in top_N_ids]))\n",
    "\n",
    "remove_indexes = top_N_ids+appears_less_than_M_times\n",
    "corpus_matrix_filtered = np.delete(corpus_matrix,remove_indexes,1)\n",
    "\n",
    "for index in sorted(remove_indexes, reverse=True):\n",
    "    del vocab_dense[index]\n",
    "\n",
    "id2word_filtered = {}\n",
    "word2id_filtered = {}\n",
    "\n",
    "for i,word in enumerate(vocab_dense):\n",
    "    id2word_filtered[i] = word\n",
    "    word2id_filtered[word] = i\n",
    "    \n",
    "corpus_filtered = gensim.matutils.Dense2Corpus(corpus_matrix_filtered, documents_columns=False)\n",
    "\n",
    "print(\"Original matrix shape:\",corpus_matrix.shape)\n",
    "print(\"New matrix shape:\",corpus_matrix_filtered.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "- Now, run LDA again using this new matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code",
    "id": "mRVVvxfwprwY"
   },
   "outputs": [],
   "source": [
    "lda = gensim.models.LdaModel(corpus_filtered, id2word=id2word_filtered, num_topics=6)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "- We can also use this model to get topic probabilities for unseen documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code",
    "id": "mRVVvxfwprwY"
   },
   "outputs": [],
   "source": [
    "unseen_doc = \"I went to the baseball game and say the player hit a homerun !\"\n",
    "unseen_doc_bow = [word2id_filtered.get(word.lower(),-1) for word in unseen_doc.split()]\n",
    "unseen_doc_vec = np.zeros(len(word2id_filtered))\n",
    "for word in unseen_doc_bow:\n",
    "    if word >= 0:\n",
    "        unseen_doc_vec[word] += 1\n",
    "unseen_doc_vec = unseen_doc_vec[np.newaxis]\n",
    "unseen_doc_corpus = gensim.matutils.Dense2Corpus(unseen_doc_vec, documents_columns=False)\n",
    "vector = lda[unseen_doc_corpus]  # get topic probability distribution for a document\n",
    "for item in vector:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "- pyLDAvis is a nice tool for visualizing our topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code",
    "id": "mRVVvxfwprwY"
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# need to create a gensim dictionary object instead of our\n",
    "# lightweight dict object - this is what pyLDA expects as input\n",
    "dictionary = gensim.corpora.Dictionary()\n",
    "dictionary.token2id = word2id_filtered\n",
    "\n",
    "# visualize the LDA model\n",
    "vis = pyLDAvis.gensim_models.prepare(lda, corpus_filtered, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For a fast and easy content analysis, use one of the many available prebuilt dictionaries/lexicons.\n",
    "    - These map words or stems to semantic categories.\n",
    "- We have discussed several lexicons in the slides.\n",
    "- As an example, let's load the lexicon for measuring personal values in text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexicon(lexicon_file_path):\n",
    "    word2cat = collections.defaultdict(list)\n",
    "    with open(lexicon_file_path,'r') as lexicon_file:\n",
    "        for line in lexicon_file:\n",
    "            if line:\n",
    "                word, cat = line.strip().split(\", \")\n",
    "                word2cat[word].append(cat)\n",
    "    return word2cat\n",
    "            \n",
    "values_lexicon = load_lexicon(\"values_lexicon.txt\")\n",
    "print(\"Loaded lexicon with\",len(values_lexicon),\"entries.\")\n",
    "print(\"The categories for 'mother' are:\",values_lexicon['mother'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very easy to score a document for each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [\"christian_500.txt\", \"business_500.txt\", \"college_500.txt\"]\n",
    "\n",
    "for file_name in file_list:\n",
    "    category_counts = collections.defaultdict(int)\n",
    "    \n",
    "    # just look at the first 25K characters\n",
    "    # this way, we don't need to normalize based on the length of the document\n",
    "    # and we'll save some time since this is just for demonstration purposes\n",
    "    text = open(file_name).read().lower()[:25000]\n",
    "    for pattern, categories in values_lexicon.items():\n",
    "        count = re.findall(r'\\b' + pattern + r'\\b', text)\n",
    "        if count:\n",
    "            for category in categories:\n",
    "                category_counts[category] += len(count)\n",
    "    print(file_name,sorted(category_counts.items(),key=lambda x:x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## A - Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup (click the \"run\" button to the left) {display-mode: \"form\"}\n",
    "\n",
    "## Setup ##\n",
    "\n",
    "# imports\n",
    "\n",
    "# built-in Python libraries\n",
    "# -------------------------\n",
    "\n",
    "# for defaultdict data structure\n",
    "import collections\n",
    "# for reading csv files\n",
    "import csv\n",
    "# operating system functions\n",
    "import os\n",
    "\n",
    "# 3rd party libraries\n",
    "# -------------------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "! pip install --upgrade gensim\n",
    "import gensim\n",
    "import gensim.test.utils\n",
    "import gensim.scripts.glove2word2vec\n",
    "import gensim.models.fasttext\n",
    "\n",
    "import scipy.spatial.distance\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "! pip install fasttext\n",
    "import fasttext\n",
    "\n",
    "if not os.path.exists(\"glove.twitter.27B.zip\"):\n",
    "    !wget http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "else:\n",
    "    print(\"GloVe already downloaded.\")\n",
    "if not os.path.exists(\"glove.twitter.27B.50d.txt\"):\n",
    "    !unzip glove.twitter.27B.zip\n",
    "else:\n",
    "    print(\"GloVe already extracted.\")\n",
    "if not os.path.exists(\"questions-words.txt\"):\n",
    "    !wget http://download.tensorflow.org/data/questions-words.txt\n",
    "else:\n",
    "    print(\"Word analogies data already loaded.\")\n",
    "if not os.path.exists(\"crawl-300d-2M-subword.zip\"):\n",
    "    !wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip\n",
    "else:\n",
    "    print(\"Fasttext embeddings already downloaded\")\n",
    "if not os.path.exists(\"crawl-300d-2M-subword.vec\"):\n",
    "    print(\"Extracting Fasttext embeddings. This may take several minutes...\")\n",
    "    !unzip crawl-300d-2M-subword.zip\n",
    "else:\n",
    "    print(\"Fasttext already extracted.\")\n",
    "if not os.path.exists(\"Stsbenchmark.tar.gz\"):\n",
    "    !wget http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz\n",
    "    !tar -xzf Stsbenchmark.tar.gz\n",
    "print()\n",
    "print(\"Done with setup!\")\n",
    "print(\"If you'd like, you can click the (X) button to the left to clear this output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using gensim for word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gensim is back: this time we'll use it to experiment with word embeddings.\n",
    "- We can use it to load an embeddings matrix in several formats.\n",
    "    - Since we've been working with some social media data, let's load the GloVe Twitter embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 100-dimensional vectors\n",
    "glove_file_path = \"glove.twitter.27B.100d.txt\"\n",
    "# First convert to word2vec format (what gensim expects)\n",
    "tmp_file_path = gensim.test.utils.get_tmpfile(\"word2vec_twitter_100d.txt\")\n",
    "_ = gensim.scripts.glove2word2vec.glove2word2vec(glove_file_path, tmp_file_path)\n",
    "\n",
    "# Now load with gensim (may take several minutes)\n",
    "print(\"Loading GloVe...\")\n",
    "glove = gensim.models.KeyedVectors.load_word2vec_format(tmp_file_path)\n",
    "print(\"GloVe was loaded into memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Now, what can we do with these embeddings?**\n",
    "- Find the most similar words to a query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.most_similar(\"france\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Measure similarity between any pair of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [ ('data','pastry') , ('beautiful','lovely'), ('hot','cold'), \\\n",
    "          ('a person walked their dog','a dog walked with a person') , \\\n",
    "          ('the school was prestigious','the university was highly ranked')]\n",
    "\n",
    "for pair in pairs:\n",
    "    sim = glove.n_similarity(pair[0].split(),pair[1].split())\n",
    "    print(pair,\":\",sim)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Or just get the vector representation for a word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = glove['summer']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The classic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystery_word_vec = glove['king'] - glove['man'] + glove['woman']\n",
    "glove.similar_by_vector(mystery_word_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it together: word analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the not-too-distance past, these types of questions appeared on American college entrance examinations:\n",
    "    - e.g., \"man:king::woman:?\"\n",
    "        - answer: queen\n",
    "- Around 2013, when word vectors were becoming extremely popular, this task was studied within the NLP community, and along with it came some standard datasets.\n",
    "    Let's load one of them here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogies = collections.defaultdict(list)\n",
    "category = \"\"\n",
    "with open(\"questions-words.txt\",'r') as analogy_questions:\n",
    "    for aq in analogy_questions:\n",
    "        if aq.startswith(\":\"):\n",
    "            category = aq.split(\":\")[1].strip()\n",
    "        else:\n",
    "            a,b,c,d = aq.split()\n",
    "            analogies[category].append([a,b,c,d])\n",
    "            \n",
    "print(\"Loaded analogies with\",len(analogies),\"categories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**: Word analogies\n",
    "- Using the GloVe embeddings that we loaded before, write a function to complete word analogies.\n",
    "    - return a list of the top `top_n` guesses, in order from most likely to least likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve an analogy a is to b as c is to ?\n",
    "def guess_analogy(a,b,c,vectors,top_n=5):\n",
    "    \n",
    "# ------------- Exercise 6 -------------- #\n",
    "\n",
    "\n",
    "    return []\n",
    "# ---------------- End ------------------ #\n",
    "    \n",
    "\n",
    "# quick test\n",
    "a,b,c,d = \"man\",\"king\",\"woman\",\"queen\"\n",
    "guesses = guess_analogy(a,b,c,glove)\n",
    "if d in guesses:\n",
    "    print(\"queen was in the top n guesses!\")\n",
    "else:\n",
    "    print(\"queen was NOT in the top n guesses...\")\n",
    "    print(\"guesses were:\",guesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Sample Solution (double-click to view) {display-mode: \"form\"}\n",
    "\n",
    "def guess_analogy(a,b,c,vectors,top_n=5):\n",
    "    \n",
    "# ------------- Exercise 6 -------------- #\n",
    "\n",
    "    inputs = set([a,b,c])\n",
    "    guess_vector = vectors[b] - vectors[a] + vectors[c]\n",
    "    guesses = glove.similar_by_vector(guess_vector)\n",
    "    return [item[0] for item in guesses if item[0] not in inputs][:top_n]\n",
    "\n",
    "# ---------------- End ------------------ #\n",
    "\n",
    "\n",
    "# quick test\n",
    "a,b,c,d = \"man\",\"king\",\"woman\",\"queen\"\n",
    "guesses = guess_analogy(a,b,c,glove)\n",
    "if d in guesses:\n",
    "    print(\"queen was in the top n guesses!\")\n",
    "else:\n",
    "    print(\"queen was NOT in the top n guesses...\")\n",
    "    print(\"guesses were:\",guesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When you are ready, run your function on some more of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "\n",
    "# just check 100 per category, otherwise you will have to wait a while\n",
    "max_per_category = 100\n",
    "correct = collections.defaultdict(int)\n",
    "top_5_correct = collections.defaultdict(int)\n",
    "total = collections.defaultdict(int)\n",
    "for category, analogy_questions in analogies.items():\n",
    "    print(\"evaluating\",category)\n",
    "    for aq in analogy_questions[:max_per_category]:\n",
    "        a,b,c,d = aq\n",
    "        if all([item in glove for item in [a,b,c,d]]):\n",
    "            guesses = guess_analogy(a,b,c,glove)\n",
    "            total[category] += 1\n",
    "            if d in guesses:\n",
    "                top_5_correct[category] += 1\n",
    "                if d == guesses[0]:\n",
    "                    correct[category] += 1\n",
    "                \n",
    "global_correct = 0\n",
    "global_top_5_correct = 0\n",
    "global_total = 0\n",
    "print()\n",
    "print(\"Category-level results\")\n",
    "for category in analogies:\n",
    "    if total[category]:\n",
    "        print(category)\n",
    "        print(\"\\t\",\"top-1 score:\",float(correct[category])/total[category])\n",
    "        print(\"\\t\",\"top-5 score:\",float(top_5_correct[category])/total[category])\n",
    "        global_correct += correct[category]\n",
    "        global_top_5_correct += top_5_correct[category]\n",
    "        global_total += total[category]\n",
    "print(\"Overall results\")\n",
    "print(\"\\t\",\"top-1 score:\",float(global_correct)/global_total)\n",
    "print(\"\\t\",\"top-5 score:\",float(global_top_5_correct)/global_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Did you get better results than the sample solution?\n",
    "    - It achieved, overall:\n",
    "        - top-1: 0.46875\n",
    "        - top-5: 0.640625\n",
    "- What could you do to improve the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B_mAiar69XVS"
   },
   "source": [
    "---\n",
    "## Sub-words and Compositionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yc6DBleGGFYk"
   },
   "source": [
    "- You may have noticed that our word embeddings don't always work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "32cQtzvXCIN9"
   },
   "outputs": [],
   "source": [
    "# note that the current version of GloVe Twitter is about 5 years old now\n",
    "try:\n",
    "    print(glove['adulting'])\n",
    "except Exception as e:\n",
    "    print(\"Error:\",e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X1Okogf3IxO8"
   },
   "source": [
    "- How could we address this?\n",
    "- We could:\n",
    "    - skip words for which we do not have embeddings\n",
    "    - use the same \"out of vocabulary\" (OOV) vector to represent each unknown word.\n",
    "    - re-train our word embeddings every...\n",
    "        - year? month? day?\n",
    "    - change the unit of semantics from *words* to *something else*.\n",
    "        - What else might we choose?\n",
    "            - Some proposals include subwords, word pieces, and characters.\n",
    "- Let's consider the last approach, first by looking at subword embeddings with fasttext:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n1ExQY49CBvJ"
   },
   "source": [
    "### Sub-words embeddings with FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d5OrZZOvKDX8"
   },
   "outputs": [],
   "source": [
    "# we can also load these using gensim\n",
    "# this will also take several minutes to load into memory\n",
    "emb_path = \"crawl-300d-2M-subword.bin\"\n",
    "fasttext_model = fasttext.load_model(emb_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E2UI57GtMC4J"
   },
   "source": [
    "- We can do the same kinds of things that we did before with GloVe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tllsXn1XMGhg"
   },
   "outputs": [],
   "source": [
    "print(\"First 50 dimensions of vector for computer:\")\n",
    "print(fasttext_model['computer'][:50])\n",
    "\n",
    "print(\"Similarity between 'forest' and 'trees':\")\n",
    "print(1- scipy.spatial.distance.cosine(fasttext_model['forest'], fasttext_model['trees']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hd4YHEacRB6Y"
   },
   "source": [
    "- In addition, we can use subword information to reason about unknown words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VjH7U3gARHJ1"
   },
   "outputs": [],
   "source": [
    "print(\"First 50 dimensions of vector for adulting:\")\n",
    "print(fasttext_model['adulting'][:50])\n",
    "print()\n",
    "# just to prove that totally new words are handled\n",
    "print(\"First 50 dimensions of vector for howhoozaling:\")\n",
    "print(fasttext_model['howhoozaling'][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BnFyVLQ8CJM7"
   },
   "source": [
    "### Semantic compositions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JmBWd9yXL3xq"
   },
   "source": [
    "- Simply averaging word embeddings (mean pooling) turns out to be a strong baseline for short text representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ljAMMzl2CO1-"
   },
   "outputs": [],
   "source": [
    "sent1 = \"The airplane flew over the fields\"\n",
    "sent2 = \"A train crossed the river\"\n",
    "\n",
    "sent1_emb = np.mean([fasttext_model[w.lower()] for w in sent1.split()],axis=0)\n",
    "sent2_emb = np.mean([fasttext_model[w.lower()] for w in sent2.split()],axis=0)\n",
    "\n",
    "print(\"Similarity:\",1- scipy.spatial.distance.cosine(sent1_emb,sent2_emb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FOkb06JGCRc9"
   },
   "source": [
    "### Putting it together: short text similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rjqKfik8rfbE"
   },
   "source": [
    "- SemEval is a yearly competition to solve a range of semantic NLP tasks.\n",
    "- A common SemEval task is \"semantic text similarity\".\n",
    "    - The goal is to build a model that can produce similarity scores for pairs of texts that are highly correlated with human judgements of similarity.\n",
    "    - Let's load some data for this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XHwjDInJsLtn"
   },
   "outputs": [],
   "source": [
    "test_sts_data = []\n",
    "fnames = ['section1','section2','section3','docid','score','doc1','doc2']\n",
    "with open(\"stsbenchmark/sts-test.csv\",'r') as infile:\n",
    "    reader = csv.DictReader(infile, fieldnames=fnames,dialect=csv.excel_tab)\n",
    "    for row in reader:\n",
    "        test_sts_data.append(row)\n",
    "print(\"Loaded\",len(test_sts_data),\"test pairs.\")\n",
    "print(\"Example:\",test_sts_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GO3Ui7vLsMKa"
   },
   "source": [
    "- Now, let's build a simple system to get some reasonable results on this task.\n",
    "    - hint: stopword removal should be helpful here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y7YJTljkCZyc"
   },
   "outputs": [],
   "source": [
    "def get_text_sim_score(text1,text2,word_embeddings):\n",
    "\n",
    "# ------------- Exercise 3 -------------- #\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- End ------------------ #\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-1y4PvfMyMMt"
   },
   "outputs": [],
   "source": [
    "#@title Sample Solution (double-click to view) {display-mode: \"form\"}\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def get_text_sim_score(text1,text2,word_embeddings):\n",
    "\n",
    "# ------------- Exercise 3 -------------- #\n",
    "\n",
    "    vec1 = np.mean([word_embeddings[w] for w in text1.split() if w.lower() not in stopwords],axis=0)\n",
    "    vec2 = np.mean([word_embeddings[w] for w in text2.split() if w.lower() not in stopwords],axis=0)\n",
    "    score = 1 - scipy.spatial.distance.cosine(vec1,vec2)\n",
    "\n",
    "# ---------------- End ------------------ #\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p31HO9h5tYs5"
   },
   "source": [
    "- Let's evaluate the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9CPvttc_tcoI"
   },
   "outputs": [],
   "source": [
    "golds, preds = [],[]\n",
    "for test_data in test_sts_data:\n",
    "    text1 = test_data['doc1']\n",
    "    text2 = test_data['doc2']\n",
    "    if text1 and text2:\n",
    "        gold = float(test_data['score'])\n",
    "        pred = get_text_sim_score(text1,text2,fasttext_model)\n",
    "        golds.append(gold)\n",
    "        preds.append(pred)\n",
    "\n",
    "print(\"Some examples of pairs and their human annotated scores:\")\n",
    "for item in test_sts_data[:5]:\n",
    "    print(item['score'],item['doc1'],item['doc2'])\n",
    "print(\"Correct labels:\",golds[:5])\n",
    "print(\"Predictions:\",preds[:5])\n",
    "print(\"Correlation Score (rho, p-value):\",scipy.stats.pearsonr(golds,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uXc7bqB5SRLY"
   },
   "source": [
    "- That is a good start! \n",
    "- It's definitely possible to do better with different compisition functions/networks, or even this same approach with different embeddings.\n",
    "    - See some of the state-of-the-art results here: http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "05_Final.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
