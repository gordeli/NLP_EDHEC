{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gordeli/NLP_EDHEC/blob/main/colab/04_Content_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "APfI_c8B40Vn"
   },
   "source": [
    "#Natural Language Processing @ EDHEC, 2022\n",
    "\n",
    "# Part 4: Content Analysis\n",
    "\n",
    "[<- Previous: Corpus Level Processing](https://colab.research.google.com/github/gordeli/NLP_EDHEC/blob/main/colab/03_Corpus_Level_Processing.ipynb)\n",
    "\n",
    "[-> Next: Sentiment Analysis](https://colab.research.google.com/github/gordeli/NLP_EDHEC/blob/main/colab/05_Sentiment_Analysis.ipynb)\n",
    "\n",
    "Dates: January 31 - February 11, 2022\n",
    "\n",
    "Facilitator: [Ivan Gordeliy](https://www.linkedin.com/in/gordeli/)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "text",
    "id": "cdTajgZhkGWX"
   },
   "source": [
    "## Initial Setup\n",
    "\n",
    "- **Run \"Setup\" below first.**\n",
    "\n",
    "    - This will load libraries and download some resources that we'll use throughout the tutorial.\n",
    "\n",
    "    - You will see a message reading \"Done with setup!\" when this process completes.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "cdTajgZhkGWX"
   },
   "outputs": [],
   "source": [
    "#@title Setup (click the \"run\" button to the left) {display-mode: \"form\"}\n",
    "\n",
    "## Setup ##\n",
    "\n",
    "# imports\n",
    "\n",
    "# built-in Python libraries\n",
    "# -------------------------\n",
    "\n",
    "# counting and data management\n",
    "import collections\n",
    "# operating system utils\n",
    "import os\n",
    "# regular expressions\n",
    "import re\n",
    "# additional string functions\n",
    "import string\n",
    "# system utilities\n",
    "import sys\n",
    "# request() will be used to load web content\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "# 3rd party libraries\n",
    "# -------------------\n",
    "\n",
    "# Natural Language Toolkit (https://www.nltk.org/)\n",
    "import nltk\n",
    "\n",
    "# download punctuation related NLTK functions\n",
    "# (needed for sent_tokenize())\n",
    "nltk.download('punkt')\n",
    "# download NLKT part-of-speech tagger\n",
    "# (needed for pos_tag())\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# download wordnet\n",
    "# (needed for lemmatization)\n",
    "nltk.download('wordnet')\n",
    "# download stopword lists\n",
    "# (needed for stopword removal)\n",
    "nltk.download('stopwords')\n",
    "# dictionary of English words\n",
    "nltk.download('words')\n",
    "\n",
    "# numpy: matrix library for Python\n",
    "import numpy as np\n",
    "\n",
    "# scipy: scientific operations\n",
    "# works with numpy objects\n",
    "import scipy\n",
    "\n",
    "# matplotlib (and pyplot) for visualizations\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn for basic machine learning operations\n",
    "import sklearn\n",
    "import sklearn.manifold\n",
    "import sklearn.cluster\n",
    "\n",
    "# worldcloud tool\n",
    "!pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# for checking object memory usage\n",
    "!pip install pympler\n",
    "from pympler import asizeof\n",
    "\n",
    "!pip install spacy\n",
    "import spacy\n",
    "\n",
    "# Downloading data\n",
    "# ----------------\n",
    "if not os.path.exists(\"aclImdb\"):\n",
    "    !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    !tar -xzf aclImdb_v1.tar.gz\n",
    "\n",
    "def text_to_lemma_frequencies(text, remove_stop_words=True):\n",
    "    \n",
    "    # split document into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # create a place to store (word, pos_tag) tuples\n",
    "    words_and_pos_tags = []\n",
    "    \n",
    "    # get all words and pos tags\n",
    "    for sentence in sentences:\n",
    "        words_and_pos_tags += nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "        \n",
    "    # load the lemmatizer\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # lemmatize the words\n",
    "    lemmas = [lemmatizer.lemmatize(word,lookup_pos(pos)) for \\\n",
    "              (word,pos) in words_and_pos_tags]\n",
    "    \n",
    "    # convert to lowercase\n",
    "    lowercase_lemmas = [lemma.lower() for lemma in lemmas]\n",
    "    \n",
    "    # load the stopword list for English\n",
    "    stop_words = set([])\n",
    "    if remove_stop_words:\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    # add punctuation to the set of things to remove\n",
    "    all_removal_tokens = stop_words | set(string.punctuation)\n",
    "    \n",
    "    # bonus: also add some custom double-quote tokens to this set\n",
    "    all_removal_tokens |= set([\"''\",\"``\"])\n",
    "    \n",
    "    # only get lemmas that aren't in these lists\n",
    "    content_lemmas = [lemma for lemma in lowercase_lemmas \\\n",
    "                      if lemma not in all_removal_tokens]\n",
    "    \n",
    "    # return the frequency distribution object\n",
    "    return nltk.probability.FreqDist(content_lemmas)\n",
    "    \n",
    "# Lemmatization -- redefining this here to make\n",
    "# code block more self-contained\n",
    "def lookup_pos(pos):\n",
    "    pos_first_char = pos[0].lower()\n",
    "    if pos_first_char in 'nv':\n",
    "        return pos_first_char\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eRm3uG2Gt9oZ"
   },
   "source": [
    "---\n",
    "## Corpus-level Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qEaT-4UAwXCk"
   },
   "source": [
    "### Matrix Representations\n",
    "\n",
    "- Representing documents as vectors of words gets us one step closer to using traditional data science approaches.\n",
    "\n",
    "- However, never forget that we're still working with language data!\n",
    "\n",
    "- **How do we get a corpus matrix?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JtgHwyi4uHi3"
   },
   "source": [
    "\n",
    "- First, we'll load a small corpus into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VvNorDdMxQ8Y"
   },
   "outputs": [],
   "source": [
    "# from the Stanford Movie Reviews Data: \n",
    "# http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "# we downloaded this during our initial Setup\n",
    "movie_review_dir = \"aclImdb/train/unsup/\"\n",
    "movie_review_files = os.listdir(movie_review_dir)\n",
    "n_movie_reviews = []\n",
    "n = 50\n",
    "for txt_file_path in sorted(movie_review_files, \\\n",
    "                            key=lambda x:int(x.split('_')[0]))[:n]:\n",
    "        full_path = movie_review_dir + txt_file_path\n",
    "        with open(full_path,'r') as txt_file:\n",
    "            n_movie_reviews.append(txt_file.read())\n",
    "            \n",
    "print(\"Loaded\",len(n_movie_reviews),\"movie reviews from the Stanford IMDB \" + \\\n",
    "      \"corpus into memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vSzT0qoQxhmm"
   },
   "source": [
    "- Start by getting a bag-of-words representation for each review.\n",
    "- Then, create a mapping between the full vocabulary and columns for our matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ap8M_6mBwV5f"
   },
   "outputs": [],
   "source": [
    "review_frequency_distributions = []\n",
    "\n",
    "# process each review, one at a time\n",
    "for review in n_movie_reviews:\n",
    "    \n",
    "    # let's use our function from before\n",
    "    frequencies = text_to_lemma_frequencies(review)\n",
    "    review_frequency_distributions.append(frequencies)\n",
    "\n",
    "# use a dictionary for faster lookup\n",
    "vocab2index = {}\n",
    "latest_index = 0\n",
    "for rfd in review_frequency_distributions:\n",
    "    for token in rfd.keys():\n",
    "        if token not in vocab2index:\n",
    "            vocab2index[token] = latest_index\n",
    "            latest_index += 1\n",
    "    \n",
    "print(\"Built vocab lookup for vocab of size:\",len(vocab2index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u0NkAsWbu0Do"
   },
   "source": [
    "- Given the frequencies and this index lookup, we can build a frequency matrix (as a numpy array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6NUN92WVu8zU"
   },
   "outputs": [],
   "source": [
    "# make an all-zero numpy array with shape n x v\n",
    "# n = number of documents\n",
    "# v = vocabulary size\n",
    "corpus_matrix = np.zeros((len(review_frequency_distributions), len(vocab2index)))\n",
    "\n",
    "# fill in the numpy array\n",
    "for row, rfd in enumerate(review_frequency_distributions):\n",
    "    for token, frequency in rfd.items():\n",
    "        column = vocab2index[token]\n",
    "        corpus_matrix[row][column] = frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MoZ7HKOKzRBX"
   },
   "outputs": [],
   "source": [
    "# get some basic information about our matrix\n",
    "def print_matrix_info(m):\n",
    "    print(\"Our corpus matrix is\",m.shape[0],'x',m.shape[1])\n",
    "    print(\"Sparsity is:\",round(float(100 * np.count_nonzero(m))/ \\\n",
    "                           (m.shape[0] * m.shape[1]),2),\"%\")\n",
    "\n",
    "print_matrix_info(corpus_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "koO0_3BSxg_p"
   },
   "source": [
    "- Now that we've seen how this works, let's see how some existing Python functions can do the heavy lifting for us.\n",
    "- Scikit learn has some useful feature extraction methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FpsI7crJxnPu"
   },
   "outputs": [],
   "source": [
    "# we can get a similar corpus matrix with just 3 lines of code\n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer()\n",
    "sklearn_corpus_data = vectorizer.fit_transform(n_movie_reviews)\n",
    "sklearn_corpus_matrix = sklearn_corpus_data.toarray()\n",
    "\n",
    "# get the feature names (1:1 mapping to the columns in the matrix)\n",
    "print(\"First 10 features:\",vectorizer.get_feature_names_out()[:10])\n",
    "print()\n",
    "\n",
    "# let's check out the matrix\n",
    "print_matrix_info(sklearn_corpus_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8WeZEZ5pyO5I"
   },
   "source": [
    "### Document Retrieval and Similarity\n",
    "\n",
    "- With this matrix, it's very easy to find all documents containing a specific word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lRJYmn4WylNW"
   },
   "outputs": [],
   "source": [
    "search_term = \"funny\"\n",
    "if search_term in vocab2index:\n",
    "    search_index = vocab2index[search_term]\n",
    "    matches = [i for i in range(corpus_matrix.shape[0]) \\\n",
    "           if corpus_matrix[i][search_index]!=0]\n",
    "\n",
    "    # list the documents that contain the search term\n",
    "    print(\"These documents contain '\"+search_term+\"':\",matches)\n",
    "    print()\n",
    "\n",
    "    # show excerpt where this word appears\n",
    "    example_location = n_movie_reviews[matches[0]].find(search_term)\n",
    "    start,end = max(example_location-30,0), min(example_location+30,len(n_movie_reviews[matches[0]]))\n",
    "    print('For example: \"...',n_movie_reviews[matches[0]][start:end],'...\"')\n",
    "    \n",
    "else:\n",
    "    print(search_term,\"isn't in the sample corups.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Ar0P-szymQg"
   },
   "source": [
    "- We can even use the notion of vector representations to compute the similarity between two documents.\n",
    "\n",
    "    - (we'll talk about more advanced ways to approach this task later in the tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wRGicr44CIGC"
   },
   "outputs": [],
   "source": [
    "example_docs =[ \"My dog likes to eat vegetables\",\\\n",
    "                \"Your dog likes to eat fruit\",\\\n",
    "                \"The computer is offline\",\\\n",
    "                \"A computer shouldn't be offline\" ]\n",
    "\n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer()\n",
    "example_data = vectorizer.fit_transform(example_docs)\n",
    "example_matrix = example_data.toarray()\n",
    "\n",
    "sim_0_1 = 1-scipy.spatial.distance.cosine(example_matrix[0],example_matrix[1])\n",
    "sim_2_3 = 1-scipy.spatial.distance.cosine(example_matrix[2],example_matrix[3])\n",
    "sim_0_2 = 1-scipy.spatial.distance.cosine(example_matrix[0],example_matrix[2])\n",
    "\n",
    "print(\"Similarity between 0 and 1:\",round(sim_0_1,2))\n",
    "print(\"Similarity between 2 and 3:\",round(sim_2_3,2))\n",
    "print(\"Similarity between 0 and 2:\",round(sim_0_2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYG5eIG7CKqZ"
   },
   "source": [
    "- We can do the same thing with our corpus of movie reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x0QOEBZVy0ME"
   },
   "outputs": [],
   "source": [
    "# choose a document, and find the most \"similar\" other document in the corpus\n",
    "reference_doc = 0\n",
    "ref_doc_vec = corpus_matrix[reference_doc]\n",
    "sim_to_ref_doc = []\n",
    "for row in corpus_matrix:\n",
    "    sim_to_ref_doc.append(1-scipy.spatial.distance.cosine(ref_doc_vec,row))\n",
    "    \n",
    "print(\"similarity scores:\",sim_to_ref_doc)\n",
    "most_similar = sim_to_ref_doc.index(max(sim_to_ref_doc[1:]))\n",
    "print(n_movie_reviews[0])\n",
    "print(\"is most similar to\")\n",
    "print(n_movie_reviews[most_similar])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kJWL6_y4MZzO"
   },
   "source": [
    "**Exercise 4**\n",
    "\n",
    "- First, let's load a dataset that should exhibit some natural groupings based on topic.\n",
    "    - [20news](http://qwone.com/~jason/20Newsgroups/) is classic NLP dataset for document classification.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "kJWL6_y4MZzO"
   },
   "outputs": [],
   "source": [
    "# load 20 newsgroups dataset - just 100 texts from 3 categories\n",
    "categories = ['comp.sys.ibm.pc.hardware', 'rec.sport.baseball']\n",
    "newsgroups_train_all = sklearn.datasets.fetch_20newsgroups(subset='train',\\\n",
    "                                                 categories=categories)\n",
    "newsgroups_train = newsgroups_train_all.data[:100]\n",
    "newsgroups_labels = newsgroups_train_all.target[:100]\n",
    "\n",
    "print(\"Loaded\",len(newsgroups_train),\"documents.\")\n",
    "print(\"Label distribution:\",collections.Counter(newsgroups_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kJWL6_y4MZzO"
   },
   "source": [
    "- Now, write a function that creates a corpus matrix from a list of strings containing documents.\n",
    "    - We can use the `text_to_lemma_frequencies` that you wrote earlier as a starting point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SkQOyk23Ll1Z"
   },
   "outputs": [],
   "source": [
    "# ------------- Exercise 2 -------------- #\n",
    "def docs2matrix(document_list):\n",
    "    \n",
    "    # this should be a nice starting point\n",
    "    lemma_freqs = [text_to_lemma_frequencies(doc) for doc in document_list]\n",
    "\n",
    "    # change this to return a 2d numpy array\n",
    "    return None\n",
    "\n",
    "# -------------     End    -------------- #\n",
    "\n",
    "# quick test with first 10 documents\n",
    "X = docs2matrix(newsgroups_train[:10])\n",
    "if type(X) != type(np.zeros([3,3])):\n",
    "    print(\"Did not return a 2d numpy matrix.\")\n",
    "elif X.shape[0] != 10:\n",
    "    print(\"number of rows should be 10, but is\",X.shape[0])\n",
    "else:\n",
    "    print(\"Created a matrix with shape:\",X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJy4zWGT4uHi"
   },
   "outputs": [],
   "source": [
    "#@title Sample Solution (double-click to view) {display-mode: \"form\"}\n",
    "\n",
    "def docs2matrix(document_list):\n",
    "    \n",
    "    # use the vocab2index idea from before\n",
    "    vocab2index = {}\n",
    "    \n",
    "    # this should be a nice starting point\n",
    "    lemma_freqs = [text_to_lemma_frequencies(doc) for doc in document_list]\n",
    "\n",
    "    latest_index = 0\n",
    "    for lf in lemma_freqs:\n",
    "        for token in lf.keys():\n",
    "            if token not in vocab2index:\n",
    "                vocab2index[token] = latest_index\n",
    "                latest_index += 1\n",
    "    \n",
    "    # create the zeros matrix\n",
    "    corpus_matrix = np.zeros((len(lemma_freqs), len(vocab2index)))\n",
    "    \n",
    "    for row, lf in enumerate(lemma_freqs):\n",
    "        for token, frequency in lf.items():\n",
    "            column = vocab2index[token]\n",
    "            corpus_matrix[row][column] = frequency\n",
    "    \n",
    "    # change this to return a 2d numpy array\n",
    "    return corpus_matrix\n",
    "\n",
    "\n",
    "# quick test with first 10 documents\n",
    "X = docs2matrix(newsgroups_train[:10])\n",
    "if type(X) != type(np.zeros([3,3])):\n",
    "    print(\"Did not return a 2d numpy matrix.\")\n",
    "elif X.shape[0] != 10:\n",
    "    print(\"number of rows should be 10, but is\",X.shape[0])\n",
    "else:\n",
    "    print(\"Created a matrix with shape:\",X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WWWn4Re11dBj"
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YLnoPCKVAvrO"
   },
   "source": [
    "- Some words are less important when making distinctions between documents in a corpus.\n",
    "- How can we determine the \"less important\" words?\n",
    "    - Using term-frequency * inverse document frequency, we make the assumption that words that appear in *many documents* are *less informative* overall.\n",
    "    - Therefore, we weigh each term based on the inverse of the number of documents that that term appears in.\n",
    "    - We can define $\\operatorname{tfidf}(t,d,D) = \\operatorname{tf}(t,d) * \\log\\frac{|D|}{|d \\in D : t \\in d|}$ , where\n",
    "        - $t$ is a term (token) in a corpus\n",
    "        - $d$ is a document in the corpus\n",
    "        - $D$ is the corpus itself, containing documents, which, in turn, contain tokens\n",
    "        - $\\operatorname{tf}(t,d)$ is the frequency of $t$ in $d$ (typically normalized at the document level).\n",
    "- sklearn has another vectorizer that takes care of this for us: the [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "    - It behaves just like the CountVectorizer() that we saw before, except it computes tfidf scores instead of counts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XFcY3Gm-JT41"
   },
   "source": [
    "- Of course we can just use the TfidfVectorizer, but what would it look like to implement this ourselves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_KSXAxnL09Dq"
   },
   "outputs": [],
   "source": [
    "# assume input matrix contains term frequencies\n",
    "def tfidf_transform(mat):\n",
    "    \n",
    "    # convert matrix of counts to matrix of normalized frequencies\n",
    "    normalized_mat = mat / np.transpose(mat.sum(axis=1)[np.newaxis])\n",
    "    \n",
    "    # compute IDF scores for each word given the corpus\n",
    "    docs_using_terms = np.count_nonzero(mat,axis=0)\n",
    "    idf_scores = np.log(mat.shape[1]/docs_using_terms)\n",
    "    \n",
    "    # compuite tfidf scores\n",
    "    tfidf_mat = normalized_mat * idf_scores\n",
    "    return tfidf_mat\n",
    "\n",
    "tfidf_X = tfidf_transform(X)\n",
    "print(\"Counts:\",X[0][0:10])\n",
    "print(\"TFIDF scores:\",tfidf_X[0][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gARK1cTYWMWU"
   },
   "source": [
    "### Bonus: SpaCy\n",
    "- If you have extra time, check out the [SpaCy 101 tutorial](https://spacy.io/usage/spacy-101)!\n",
    "    - SpaCy is less research focused, but after you have a good grasp on the core concepts, it can provide a powerful set of NLP tools, and it is definitely worth knowing about.\n",
    "        - It is also often faster to run than NLTK.\n",
    "        - (we will time our nltk version first, for reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HK1p4QRx3b33"
   },
   "outputs": [],
   "source": [
    "%timeit docs2matrix(newsgroups_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yYYxRB_1q9jA"
   },
   "outputs": [],
   "source": [
    "# Example preprocessing with SpaCy\n",
    "def text_to_lemma_frequencies(text):\n",
    "    nlp = spacy.load('en')\n",
    "    doc = nlp(text)\n",
    "    words = [token.lemma for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "    return collections.Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DW4X4ZzwsgFP"
   },
   "outputs": [],
   "source": [
    "# Example document matrix building \n",
    "X = docs2matrix(newsgroups_train)\n",
    "print(\"Created a matrix with shape:\",X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PliAhvcf3Uxl"
   },
   "outputs": [],
   "source": [
    "%timeit docs2matrix(newsgroups_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aC64NI4B4u-M"
   },
   "source": [
    "- Why so slow?\n",
    "    - SpaCy is doing too many tasks that we don't need here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AByJBR2y4225"
   },
   "outputs": [],
   "source": [
    "NLP = spacy.load('en',disable=['ner','parser'])\n",
    "def text_to_lemma_frequencies(text):    \n",
    "    doc = NLP(text)\n",
    "    words = [token.lemma for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "    return collections.Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NzO9KyBr5kgk"
   },
   "outputs": [],
   "source": [
    "%timeit docs2matrix(newsgroups_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NzO9KyBr5kgk"
   },
   "source": [
    "## Content Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "### Visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "- Let's visualize the data in 2 dimensions\n",
    "    - We'll use [T-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to do the dimensionality reduction.\n",
    "    - Each color (red and blue) will represent one of the \"ground truth\" clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "outputs": [],
   "source": [
    "# show corpus in 2d\n",
    "\n",
    "X = docs2matrix(newsgroups_train)\n",
    "print(\"Created a matrix with shape:\",X.shape)\n",
    "tsne = sklearn.manifold.TSNE(n_components=2, random_state=1)\n",
    "X_2d = tsne.fit_transform(X)\n",
    "colors = ['r', 'b']\n",
    "target_ids = range(len(categories))\n",
    "for target, c, label in zip(target_ids, colors, categories):\n",
    "    plt.scatter(X_2d[newsgroups_labels == target, 0], X_2d[newsgroups_labels == target, 1], c=c, label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "- The groups have a fair degree of overlap. Can kmeans clustering recover them correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "outputs": [],
   "source": [
    "# Do kmeans clustering\n",
    "\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters=2, random_state=0, algorithm=\"full\").fit(X)\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "for target, c, label in zip(target_ids, colors, categories):\n",
    "    plt.scatter(X_2d[clusters == target, 0], X_2d[clusters == target, 1], c=c, label=label)\n",
    "\n",
    "# out own purity function\n",
    "def compute_average_purity(clusters, labels):\n",
    "    # and computer the cluster purity\n",
    "    cluster_labels = collections.defaultdict(list)\n",
    "    for i in range(len(clusters)):\n",
    "        cluster = clusters[i]\n",
    "        label = labels[i]\n",
    "        cluster_labels[cluster].append(label)\n",
    "    cluster_purities = {}\n",
    "    for cluster, labels in cluster_labels.items():\n",
    "        most_common_count = collections.Counter(labels).most_common()[0][1]\n",
    "        purity = float(most_common_count)/len(labels)\n",
    "        cluster_purities[cluster] = purity\n",
    "    avg_purity = sum(cluster_purities.values())/len(cluster_purities.keys())\n",
    "    print(\"Average cluster purity:\",avg_purity)\n",
    "    \n",
    "avg_purity = compute_average_purity(clusters, newsgroups_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "- That didn't work as well as we'd like it to.\n",
    "- It's time to introduce better features that just word frequencies.\n",
    "    - TF-IDF to the rescue!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "- Some words are less important when making distinctions between documents in a corpus.\n",
    "- How can we determine the \"less important\" words?\n",
    "    - Using term-frequency * inverse document frequency, we make the assumption that words that appear in *many documents* are *less informative* overall.\n",
    "    - Therefore, we weigh each term based on the inverse of the number of documents that that term appears in.\n",
    "    - We can define $\\operatorname{tfidf}(t,d,D) = \\operatorname{tf}(t,d) * \\log\\frac{|D|}{|d \\in D : t \\in d|}$ , where\n",
    "        - $t$ is a term (token) in a corpus\n",
    "        - $d$ is a document in the corpus\n",
    "        - $D$ is the corpus itself, containing documents, which, in turn, contain tokens\n",
    "        - $\\operatorname{tf}(t,d)$ is the frequency of $t$ in $d$ (typically normalized at the document level).\n",
    "- sklearn has another vectorizer that takes care of this for us: the [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "    - It behaves just like the CountVectorizer() that we saw before, except it computes tfidf scores instead of counts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "- Of course we can just use the TfidfVectorizer, but what would it look like to implement this ourselves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "outputs": [],
   "source": [
    "# assume input matrix contains term frequencies\n",
    "def tfidf_transform(mat):\n",
    "    \n",
    "    # convert matrix of counts to matrix of normalized frequencies\n",
    "    normalized_mat = mat / np.transpose(mat.sum(axis=1)[np.newaxis])\n",
    "    \n",
    "    # compute IDF scores for each word given the corpus\n",
    "    docs_using_terms = np.count_nonzero(mat,axis=0)\n",
    "    idf_scores = np.log(mat.shape[1]/docs_using_terms)\n",
    "    \n",
    "    # compuite tfidf scores\n",
    "    tfidf_mat = normalized_mat * idf_scores\n",
    "    return tfidf_mat\n",
    "\n",
    "tfidf_X = tfidf_transform(X)\n",
    "print(\"Counts:\",X[0][0:10])\n",
    "print(\"TFIDF scores:\",tfidf_X[0][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "- What happens if we use tfidf instead of just counts or frequencies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "outputs": [],
   "source": [
    "# show corpus in 2d\n",
    "\n",
    "#X = docs2matrix(newsgroups_train)\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(newsgroups_train).todense()\n",
    "print(\"Created a matrix with shape:\",X.shape)\n",
    "tsne = sklearn.manifold.TSNE(n_components=2, random_state=1)\n",
    "X_2d = tsne.fit_transform(X)\n",
    "colors = ['r', 'b']\n",
    "target_ids = range(len(categories))\n",
    "for target, c, label in zip(target_ids, colors, categories):\n",
    "    plt.scatter(X_2d[newsgroups_labels == target, 0], X_2d[newsgroups_labels == target, 1], c=c, label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "- These groups appear to have a bit more separation.\n",
    "- How well can kmeans recover the original groups now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "outputs": [],
   "source": [
    "# Do kmeans clustering with TF-IDF matrisx\n",
    "\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters=2, random_state=0, algorithm=\"full\").fit(X)\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "for target, c, label in zip(target_ids, colors, categories):\n",
    "    plt.scatter(X_2d[clusters == target, 0], X_2d[clusters == target, 1], c=c, label=label)\n",
    "    \n",
    "avg_purity = compute_average_purity(clusters, newsgroups_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "outputs": [],
   "source": [
    "#@title Setup (click the \"run\" button to the left) {display-mode: \"form\"}\n",
    "\n",
    "## Setup ##\n",
    "\n",
    "# imports\n",
    "\n",
    "# built-in Python libraries\n",
    "# -------------------------\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 3rd party libraries\n",
    "# -------------------\n",
    "\n",
    "# Natural Language Toolkit (https://www.nltk.org/)\n",
    "import nltk\n",
    "\n",
    "# download punctuation related NLTK functions\n",
    "# (needed for sent_tokenize())\n",
    "nltk.download('punkt')\n",
    "# download NLKT part-of-speech tagger\n",
    "# (needed for pos_tag())\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# download wordnet\n",
    "# (needed for lemmatization)\n",
    "nltk.download('wordnet')\n",
    "# download stopword lists\n",
    "# (needed for stopword removal)\n",
    "nltk.download('stopwords')\n",
    "# dictionary of English words\n",
    "nltk.download('words')\n",
    "\n",
    "# numpy: matrix library for Python\n",
    "import numpy as np\n",
    "\n",
    "!pip install -U gensim\n",
    "\n",
    "# Gensim for topic modeling\n",
    "import gensim\n",
    "# for loading data\n",
    "import sklearn.datasets\n",
    "# for LDA visualization\n",
    "!pip install pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "# for uploading data files\n",
    "from google.colab import files\n",
    "\n",
    "# downloading values lexicon\n",
    "!wget https://raw.githubusercontent.com/steve-wilson/values_lexicon/master/lexicon_1_0/values_lexicon.txt\n",
    "!wget https://raw.githubusercontent.com/steve-wilson/values_lexicon/master/sample_data/subreddits/christian_500.txt\n",
    "!wget https://raw.githubusercontent.com/steve-wilson/values_lexicon/master/sample_data/subreddits/business_500.txt\n",
    "!wget https://raw.githubusercontent.com/steve-wilson/values_lexicon/master/sample_data/subreddits/college_500.txt\n",
    "\n",
    "def text_to_lemma_frequencies(text, remove_stop_words=True):\n",
    "    \n",
    "    # split document into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # create a place to store (word, pos_tag) tuples\n",
    "    words_and_pos_tags = []\n",
    "    \n",
    "    # get all words and pos tags\n",
    "    for sentence in sentences:\n",
    "        words_and_pos_tags += nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "        \n",
    "    # load the lemmatizer\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # lemmatize the words\n",
    "    lemmas = [lemmatizer.lemmatize(word,lookup_pos(pos)) for \\\n",
    "              (word,pos) in words_and_pos_tags]\n",
    "    \n",
    "    # convert to lowercase\n",
    "    lowercase_lemmas = [lemma.lower() for lemma in lemmas]\n",
    "    \n",
    "    # load the stopword list for English\n",
    "    stop_words = set([])\n",
    "    if remove_stop_words:\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    # add punctuation to the set of things to remove\n",
    "    all_removal_tokens = stop_words | set(string.punctuation)\n",
    "    \n",
    "    # bonus: also add some custom double-quote tokens to this set\n",
    "    all_removal_tokens |= set([\"''\",\"``\"])\n",
    "    \n",
    "    # only get lemmas that aren't in these lists\n",
    "    content_lemmas = [lemma for lemma in lowercase_lemmas \\\n",
    "                      if lemma not in all_removal_tokens and \\\n",
    "                      re.match(r\"^\\w+$\",lemma)]\n",
    "    \n",
    "    # return the frequency distribution object\n",
    "    return nltk.probability.FreqDist(content_lemmas)\n",
    "    \n",
    "def docs2matrix(document_list):\n",
    "    \n",
    "    # use the vocab2index idea from before\n",
    "    vocab2index = {}\n",
    "    \n",
    "    # load the stopword list for English\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    stop_words |= set(['from', 'subject', 're', 'edu', 'use'])\n",
    "    \n",
    "    # add punctuation to the set of things to remove\n",
    "    all_removal_tokens = stop_words | set(string.punctuation)\n",
    "    \n",
    "    # bonus: also add some custom double-quote tokens to this set\n",
    "    all_removal_tokens |= set([\"''\",\"``\"])\n",
    "    \n",
    "    vocab2index = {}\n",
    "    latest_index = 0\n",
    "\n",
    "    lfs = []\n",
    "    # this should be a nice starting point\n",
    "    for doc in document_list:\n",
    "        lf = text_to_lemma_frequencies(doc,all_removal_tokens)\n",
    "        for token in lf.keys():\n",
    "            if token not in vocab2index:\n",
    "                vocab2index[token] = latest_index\n",
    "                latest_index += 1\n",
    "                \n",
    "        lfs.append(lf)\n",
    "    \n",
    "    # create the zeros matrix\n",
    "    corpus_matrix = np.zeros((len(lfs), len(vocab2index)))\n",
    "    \n",
    "    for row, lf in enumerate(lfs):\n",
    "        for token, frequency in lf.items():\n",
    "            column = vocab2index[token]\n",
    "            corpus_matrix[row][column] = frequency\n",
    "    \n",
    "    return corpus_matrix, vocab2index\n",
    "\n",
    "    \n",
    "# Lemmatization -- redefining this here to make\n",
    "# code block more self-contained\n",
    "def lookup_pos(pos):\n",
    "    pos_first_char = pos[0].lower()\n",
    "    if pos_first_char in 'nv':\n",
    "        return pos_first_char\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "\n",
    "            \n",
    "print()\n",
    "print(\"Done with setup!\")\n",
    "print(\"If you'd like, you can click the (X) button to the left to clear this output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "- Now that we have some real data, what are some ways that we can explore what's in it?\n",
    "    - How can we answer the basic question: *What are people talking about in this corpus?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "- Load a corpus matrix, like the ones we created earlier, into gensim's corpus object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "outputs": [],
   "source": [
    "# this time, let's load all documents in the 20news dataset from these categories\n",
    "categories = ['soc.religion.christian', 'rec.autos', 'talk.politics.misc', \\\n",
    "              'rec.sport.baseball', 'comp.sys.ibm.pc.hardware']\n",
    "newsgroups_train_all = sklearn.datasets.fetch_20newsgroups(subset='train', \\\n",
    "                                              categories=categories).data\n",
    "# using the function we wrote before, but modified to also return the vocab2index\n",
    "corpus_matrix, word2id = docs2matrix(newsgroups_train_all)\n",
    "# reverse this dictionary\n",
    "id2word = {v:k for k,v in word2id.items()}\n",
    "\n",
    "corpus = gensim.matutils.Dense2Corpus(corpus_matrix, documents_columns=False)\n",
    "print(\"Loaded\",len(corpus),\"documents into a Gensim corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "- Given this, we can run LDA right out of the box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "outputs": [],
   "source": [
    "# As of July 2019, gensim calls a deprecated numpy function and gives lots of warning messages\n",
    "# Let's supress these.\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# run LDA on our corpus, using out dictionary (k=6)\n",
    "lda = gensim.models.LdaModel(corpus, id2word=id2word, num_topics=6)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "- There is still quite a bit of noise in this list because the documents are full of very common words like \"write\", \"subject\", and \"from\".\n",
    "- One common approach is to remove the most (and possibly least) common words before running LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "outputs": [],
   "source": [
    "total_counts = np.sum(corpus_matrix, axis=0)\n",
    "sorted_words = sorted( zip( range(len(total_counts)) ,total_counts), \\\n",
    "                       key=lambda x:x[1], reverse=True )\n",
    "N = 100\n",
    "M = 50\n",
    "top_N_ids = [item[0] for item in sorted_words[:N]]\n",
    "appears_less_than_M_times = [item[0] for item in sorted_words if item[1] < M]\n",
    "vocab_dense = [id2word[idx] for idx in range(len(id2word))]\n",
    "\n",
    "print(\"Top words to remove:\", ' '.join([id2word[idx] for idx in top_N_ids]))\n",
    "\n",
    "remove_indexes = top_N_ids+appears_less_than_M_times\n",
    "corpus_matrix_filtered = np.delete(corpus_matrix,remove_indexes,1)\n",
    "\n",
    "for index in sorted(remove_indexes, reverse=True):\n",
    "    del vocab_dense[index]\n",
    "\n",
    "id2word_filtered = {}\n",
    "word2id_filtered = {}\n",
    "\n",
    "for i,word in enumerate(vocab_dense):\n",
    "    id2word_filtered[i] = word\n",
    "    word2id_filtered[word] = i\n",
    "    \n",
    "corpus_filtered = gensim.matutils.Dense2Corpus(corpus_matrix_filtered, documents_columns=False)\n",
    "\n",
    "print(\"Original matrix shape:\",corpus_matrix.shape)\n",
    "print(\"New matrix shape:\",corpus_matrix_filtered.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "- Now, run LDA again using this new matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "outputs": [],
   "source": [
    "lda = gensim.models.LdaModel(corpus_filtered, id2word=id2word_filtered, num_topics=6)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "- We can also use this model to get topic probabilities for unseen documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "outputs": [],
   "source": [
    "unseen_doc = \"I went to the baseball game and say the player hit a homerun !\"\n",
    "unseen_doc_bow = [word2id_filtered.get(word.lower(),-1) for word in unseen_doc.split()]\n",
    "unseen_doc_vec = np.zeros(len(word2id_filtered))\n",
    "for word in unseen_doc_bow:\n",
    "    if word >= 0:\n",
    "        unseen_doc_vec[word] += 1\n",
    "unseen_doc_vec = unseen_doc_vec[np.newaxis]\n",
    "unseen_doc_corpus = gensim.matutils.Dense2Corpus(unseen_doc_vec, documents_columns=False)\n",
    "vector = lda[unseen_doc_corpus]  # get topic probability distribution for a document\n",
    "for item in vector:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "- pyLDAvis is a nice tool for visualizing our topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# need to create a gensim dictionary object instead of our\n",
    "# lightweight dict object - this is what pyLDA expects as input\n",
    "dictionary = gensim.corpora.Dictionary()\n",
    "dictionary.token2id = word2id_filtered\n",
    "\n",
    "# visualize the LDA model\n",
    "vis = pyLDAvis.gensim_models.prepare(lda, corpus_filtered, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRVVvxfwprwY"
   },
   "source": [
    "- [-> Next: Sentiment Analysis](https://colab.research.google.com/github/gordeli/NLP_EDHEC/blob/main/05_Sentiment_Analysis.ipynb)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "04_Content_Analysis.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
